{
  
    
        "post0": {
            "title": "2장 시계열 데이터의 발견과 정리 - Python",
            "content": "3.2 &#54596;&#50836;&#54620; &#54056;&#53412;&#51648; . 3.5 &#45936;&#51060;&#53552;&#51032; &#49688;&#51665; . url = &quot;https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3.xls&quot; df = pd.read_excel(url) orig_df = df . df.columns . Index([&#39;pclass&#39;, &#39;survived&#39;, &#39;name&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;ticket&#39;, &#39;fare&#39;, &#39;cabin&#39;, &#39;embarked&#39;, &#39;boat&#39;, &#39;body&#39;, &#39;home.dest&#39;], dtype=&#39;object&#39;) . 3.6 &#45936;&#51060;&#53552;&#51032; &#51221;&#47532; . df.dtypes . pclass int64 survived int64 name object sex object age float64 sibsp int64 parch int64 ticket object fare float64 cabin object embarked object boat object body float64 home.dest object dtype: object . df.shape . (1309, 14) . df.describe().iloc[:, :2] . pclass survived . count 1309.000000 | 1309.000000 | . mean 2.294882 | 0.381971 | . std 0.837836 | 0.486055 | . min 1.000000 | 0.000000 | . 25% 2.000000 | 0.000000 | . 50% 3.000000 | 0.000000 | . 75% 3.000000 | 1.000000 | . max 3.000000 | 1.000000 | . df.isnull().sum() . pclass 0 survived 0 name 0 sex 0 age 263 sibsp 0 parch 0 ticket 0 fare 1 cabin 1014 embarked 2 boat 823 body 1188 home.dest 564 dtype: int64 . mask = df.isnull().any(axis=1) . mask.head() . 0 True 1 True 2 True 3 True 4 True dtype: bool . df[mask].body.head() . 0 NaN 1 NaN 2 NaN 3 135.0 4 NaN Name: body, dtype: float64 . df.sex.value_counts(dropna=False) . male 843 female 466 Name: sex, dtype: int64 . df.embarked.value_counts(dropna=False) . S 914 C 270 Q 123 NaN 2 Name: embarked, dtype: int64 . 3.7 &#53945;&#51669;&#51032; &#49373;&#49457; . name = df.name name.head(3) . 0 Allen, Miss. Elisabeth Walton 1 Allison, Master. Hudson Trevor 2 Allison, Miss. Helen Loraine Name: name, dtype: object . df = df.drop( columns=[&quot;name&quot;, &quot;ticket&quot;, &quot;home.dest&quot;, &quot;boat&quot;, &quot;body&quot;, &quot;cabin&quot;] ) . df = pd.get_dummies(df) . df.columns . Index([&#39;pclass&#39;, &#39;survived&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;fare&#39;, &#39;sex_female&#39;, &#39;sex_male&#39;, &#39;embarked_C&#39;, &#39;embarked_Q&#39;, &#39;embarked_S&#39;], dtype=&#39;object&#39;) . df = df.drop(columns=&quot;sex_male&quot;) . df = pd.get_dummies(df, drop_first=True) . df.columns . Index([&#39;pclass&#39;, &#39;survived&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;fare&#39;, &#39;sex_female&#39;, &#39;embarked_C&#39;, &#39;embarked_Q&#39;, &#39;embarked_S&#39;], dtype=&#39;object&#39;) . y = df.survived X = df.drop(columns=&quot;survived&quot;) . 3.8 &#49368;&#54540; &#45936;&#51060;&#53552; . X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42) . 3.9 &#45936;&#51060;&#53552;&#51032; &#45824;&#52824; . from sklearn.experimental import ( enable_iterative_imputer, ) from sklearn import impute . num_cols = [ &quot;pclass&quot;, &quot;age&quot;, &quot;sibsp&quot;, &quot;parch&quot;, &quot;fare&quot;, &quot;sex_female&quot;, ] . imputer = impute.IterativeImputer() imputed = imputer.fit_transform( X_train[num_cols] ) X_train.loc[:, num_cols] = imputed imputed = imputer.transform(X_test[num_cols]) X_test.loc[:, num_cols] = imputed . meds = X_train.median() X_train = X_train.fillna(meds) X_test = X_test.fillna(meds) . X_train.head() . pclass age sibsp parch fare sex_female embarked_C embarked_Q embarked_S . 1214 3.0 | 26.984481 | 0.0 | 0.0 | 8.6625 | 0.0 | 0 | 0 | 1 | . 677 3.0 | 26.000000 | 0.0 | 0.0 | 7.8958 | 0.0 | 0 | 0 | 1 | . 534 2.0 | 19.000000 | 0.0 | 0.0 | 26.0000 | 1.0 | 0 | 0 | 1 | . 1174 3.0 | 0.437798 | 8.0 | 2.0 | 69.5500 | 1.0 | 0 | 0 | 1 | . 864 3.0 | 28.000000 | 0.0 | 0.0 | 7.7750 | 1.0 | 0 | 0 | 1 | . 3.10 &#45936;&#51060;&#53552;&#51032; &#54364;&#51456;&#54868; . cols = &quot;pclass,age,sibsp,fare&quot;.split(&quot;,&quot;) sca = preprocessing.StandardScaler() . X_train = sca.fit_transform(X_train) X_train = pd.DataFrame(X_train[:, :4], columns=cols) X_test = sca.transform(X_test) X_test = pd.DataFrame(X_test[:, :4], columns=cols) . 3.11 &#47532;&#54057;&#53552;&#47553; . def tweak_titanic(df): df = df.drop( columns=[ &quot;name&quot;, &quot;ticket&quot;, &quot;home.dest&quot;, &quot;boat&quot;, &quot;body&quot;, &quot;cabin&quot;, ] ).pipe(pd.get_dummies, drop_first=True) return df def get_train_test_X_y(df, y_col, size=0.3, std_cols=None): y = df[y_col] X = df.drop(columns=y_col) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=size, random_state=42 ) cols = X.columns num_cols = [ &quot;pclass&quot;, &quot;age&quot;, &quot;sibsp&quot;, &quot;parch&quot;, &quot;fare&quot;, ] fi = impute.IterativeImputer() X_train.loc[:, num_cols] = fi.fit_transform(X_train[num_cols]) X_test.loc[:, num_cols] = fi.transform(X_test[num_cols]) if std_cols: std = preprocessing.StandardScaler() X_train.loc[:, std_cols] = std.fit_transform(X_train[std_cols]) X_test.loc[:, std_cols] = std.transform(X_test[std_cols]) return X_train, X_test, y_train, y_test . ti_df = tweak_titanic(orig_df) std_cols = &quot;pclass,age,sibsp,fare&quot;.split(&quot;,&quot;) X_train, X_test, y_train, y_test = get_train_test_X_y(ti_df, &quot;survived&quot;, std_cols=std_cols) . 3.12 &#48288;&#51060;&#49828;&#46972;&#51064; &#47784;&#45944; . from sklearn.dummy import DummyClassifier bm = DummyClassifier() bm.fit(X_train, y_train) bm.score(X_test, y_test) # 정확도 . 0.5623409669211196 . from sklearn import metrics metrics.precision_score(y_test, bm.predict(X_test)) . 0.44 . 3.13 &#45796;&#50577;&#54620; &#50508;&#44256;&#47532;&#51608; . X = pd.concat([X_train, X_test]) y = pd.concat([y_train, y_test]) . from sklearn import model_selection from sklearn.dummy import DummyClassifier from sklearn.linear_model import ( LogisticRegression, ) from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import ( KNeighborsClassifier, ) from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC from sklearn.ensemble import ( RandomForestClassifier, ) import xgboost . for model in [ DummyClassifier, LogisticRegression, DecisionTreeClassifier, KNeighborsClassifier, GaussianNB, SVC, RandomForestClassifier, xgboost.XGBClassifier, ]: cls = model() kfold = model_selection.KFold(n_splits=10, random_state=42) s = model_selection.cross_val_score(cls, X, y, scoring=&quot;roc_auc&quot;, cv=kfold) print(f&quot;{model.__name__:22} AUC: {s.mean():.3f} STD: {s.std():.2f}&quot;) . DummyClassifier AUC: 0.523 STD: 0.03 LogisticRegression AUC: 0.843 STD: 0.03 DecisionTreeClassifier AUC: 0.762 STD: 0.03 KNeighborsClassifier AUC: 0.830 STD: 0.05 GaussianNB AUC: 0.817 STD: 0.04 SVC AUC: 0.837 STD: 0.05 RandomForestClassifier AUC: 0.845 STD: 0.03 XGBClassifier AUC: 0.863 STD: 0.04 . 3.14 &#49828;&#53468;&#53433; . from mlxtend.classifier import ( StackingClassifier, ) clfs = [ x() for x in [ LogisticRegression, DecisionTreeClassifier, KNeighborsClassifier, GaussianNB, SVC, RandomForestClassifier, ] ] stack = StackingClassifier( classifiers=clfs, meta_classifier=LogisticRegression(), ) kfold = model_selection.KFold(n_splits=10, random_state=42) s = model_selection.cross_val_score(stack, X, y, scoring=&quot;roc_auc&quot;, cv=kfold) print(f&quot;{stack.__class__.__name__} AUC: {s.mean():.3f} STD: {s.std():.2f}&quot;) . StackingClassifier AUC: 0.785 STD: 0.04 . 3.15 &#47784;&#45944; &#47564;&#46308;&#44592; . rf = ensemble.RandomForestClassifier(n_estimators=100, random_state=42) rf.fit(X_train, y_train) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) . 3.16 &#47784;&#45944;&#51032; &#54217;&#44032; . rf.score(X_test, y_test) . 0.7837150127226463 . metrics.precision_score(y_test, rf.predict(X_test)) . 0.7916666666666666 . for col, val in sorted(zip(X_train.columns, rf.feature_importances_), key=lambda x: x[1], reverse=True)[:5]: print(f&quot;{col:10}{val:10.3f}&quot;) . age 0.285 fare 0.262 sex_male 0.241 pclass 0.089 sibsp 0.050 . 3.17 &#47784;&#45944;&#51032; &#52572;&#51201;&#54868; . rf4 = ensemble.RandomForestClassifier() params = { &quot;max_features&quot;: [0.4, &quot;auto&quot;], &quot;n_estimators&quot;: [15, 200], &quot;min_samples_leaf&quot;: [1, 0.1], &quot;random_state&quot;: [42], } cv = model_selection.GridSearchCV(rf4, params, n_jobs=-1).fit(X_train, y_train) print(cv.best_params_) . {&#39;max_features&#39;: 0.4, &#39;min_samples_leaf&#39;: 1, &#39;n_estimators&#39;: 200, &#39;random_state&#39;: 42} . rf5 = ensemble.RandomForestClassifier( **{ &quot;max_features&quot;: &quot;auto&quot;, &quot;min_samples_leaf&quot;: 0.1, &quot;n_estimators&quot;: 200, &quot;random_state&quot;: 42, } ) rf5.fit(X_train, y_train) rf5.score(X_test, y_test) . 0.7073791348600509 . 3.18 &#50724;&#52264; &#54665;&#47148; . from sklearn.metrics import confusion_matrix y_pred = rf5.predict(X_test) confusion_matrix(y_test, y_pred) . array([[217, 7], [108, 61]]) . mapping = {0: &quot;died&quot;, 1: &quot;survived&quot;} fig, ax = plt.subplots(figsize=(6, 6)) cm_viz = ConfusionMatrix( rf5, classes=[&quot;died&quot;, &quot;survived&quot;], label_encoder=mapping, ) cm_viz.score(X_test, y_test) cm_viz.poof() plt.show() . 3.19 ROC &#44257;&#49440; . y_pred = rf5.predict(X_test) roc_auc_score(y_test, y_pred) . 0.6648483727810651 . fig, ax = plt.subplots(figsize=(6, 6)) roc_viz = ROCAUC(rf5) roc_viz.score(X_test, y_test) roc_viz.poof() plt.show() . 3.20 &#54617;&#49845; &#44257;&#49440; . import numpy as np fig, ax = plt.subplots(figsize=(6, 4)) cv = StratifiedKFold(12) sizes = np.linspace(0.3, 1.0, 10) lc_viz = LearningCurve( rf5, cv=cv, train_sizes=sizes, scoring=&quot;f1_weighted&quot;, n_jobs=4, ax=ax, ) lc_viz.fit(X, y) lc_viz.poof() plt.show() . 3.21 &#47784;&#45944;&#51032; &#48176;&#54252; . import pickle pic = pickle.dumps(rf5) rf6 = pickle.loads(pic) y_pred = rf6.predict(X_test) roc_auc_score(y_test, y_pred) . 0.6648483727810651 .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter3/",
            "relUrl": "/chapter3/",
            "date": " • Mar 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://deep-diver.github.io/pocket-ml-reference-korean/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://deep-diver.github.io/pocket-ml-reference-korean/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}