{
  
    
        "post0": {
            "title": "9장 불균형 범주의 문제",
            "content": "9.4 &#49548;&#49688;&#51665;&#45800; &#45936;&#51060;&#53552; &#50629;&#49368;&#54540;&#47553;&#54616;&#44592; . from sklearn.utils import resample mask = df.survived == 1 surv_df = df[mask] death_df = df[~mask] df_upsample = resample( surv_df, replace=True, n_samples=len(death_df), random_state=42, ) df2 = pd.concat([death_df, df_upsample]) df2.survived.value_counts() . 1 809 0 809 Name: survived, dtype: int64 . from imblearn.over_sampling import ( RandomOverSampler, ) ros = RandomOverSampler(random_state=42) X_ros, y_ros = ros.fit_sample(X, y) pd.Series(y_ros).value_counts() . 1 809 0 809 dtype: int64 . 9.5 &#44284;&#48152;&#49688;&#51665;&#45800; &#45936;&#51060;&#53552;&#47484; &#45796;&#50868;&#49368;&#54540;&#47553;&#54616;&#44592; . from sklearn.utils import resample mask = df.survived == 1 surv_df = df[mask] death_df = df[~mask] df_downsample = resample( death_df, replace=False, n_samples=len(surv_df), random_state=42, ) df3 = pd.concat([surv_df, df_downsample]) df3.survived.value_counts() . 1 500 0 500 Name: survived, dtype: int64 .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter9/",
            "relUrl": "/chapter9/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "8장 특징의 선택",
            "content": "8.1 &#44277;&#49440;&#49457;&#51012; &#44032;&#51652; &#50676; . import numpy as np threshold = 0.95 corr = agg_df.corr() mask = np.triu( np.ones(corr.shape), k=1 ).astype(bool) corr_no_diag = corr.where(mask) coll = [ c for c in corr_no_diag.columns if any(abs(corr_no_diag[c]) &gt; threshold) ] coll . [&#39;pclass_min&#39;, &#39;pclass_max&#39;, &#39;pclass_mean&#39;, &#39;sibsp_mean&#39;, &#39;parch_mean&#39;, &#39;fare_mean&#39;, &#39;body_min&#39;, &#39;body_max&#39;, &#39;body_mean&#39;, &#39;body_sum&#39;] . url = &quot;https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3.xls&quot; df = pd.read_excel(url) orig_df = df def tweak_titanic(df): df = df.drop( columns=[ &quot;name&quot;, &quot;ticket&quot;, &quot;home.dest&quot;, &quot;boat&quot;, &quot;body&quot;, &quot;cabin&quot;, ] ).pipe(pd.get_dummies, drop_first=True) return df def get_train_test_X_y(df, y_col, size=0.3, std_cols=None): y = df[y_col] X = df.drop(columns=y_col) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=size, random_state=42 ) cols = X.columns num_cols = [ &quot;pclass&quot;, &quot;age&quot;, &quot;sibsp&quot;, &quot;parch&quot;, &quot;fare&quot;, ] fi = impute.IterativeImputer() X_train.loc[:, num_cols] = fi.fit_transform(X_train[num_cols]) X_test.loc[:, num_cols] = fi.transform(X_test[num_cols]) if std_cols: std = preprocessing.StandardScaler() X_train.loc[:, std_cols] = std.fit_transform(X_train[std_cols]) X_test.loc[:, std_cols] = std.transform(X_test[std_cols]) return X_train, X_test, y_train, y_test ti_df = tweak_titanic(orig_df) X_train, X_test, y_train, y_test = get_train_test_X_y(ti_df, &quot;survived&quot;) . import rfpimp rfpimp.plot_dependence_heatmap( rfpimp.feature_dependence_matrix(X_train), value_fontsize=12, label_fontsize=14, figsize=(8, 8), ) fig = plt.gcf() . from sklearn.ensemble import ( RandomForestClassifier, ) cols_to_remove = [ &quot;pclass&quot;, &quot;sibsp&quot;, &quot;parch&quot;, &quot;embarked_Q&quot;, ] rf3 = RandomForestClassifier(random_state=42) rf3.fit( X_train[ [ c for c in X_train.columns if c not in cols_to_remove ] ], y_train, ) rf3.score( X_test[ [ c for c in X_train.columns if c not in cols_to_remove ] ], y_test, ) . 0.7608142493638677 . 8.2 &#46972;&#49548; &#54924;&#44480; . from sklearn import linear_model model = linear_model.LassoLarsCV( cv=10, max_n_alphas=10 ).fit(X_train, y_train) fig, ax = plt.subplots(figsize=(12, 8)) cm = iter( plt.get_cmap(&quot;tab20&quot;)( np.linspace(0, 1, X.shape[1]) ) ) for i in range(X.shape[1]): c = next(cm) ax.plot( model.alphas_, model.coef_path_.T[:, i], c=c, alpha=0.8, label=X.columns[i], ) ax.axvline( model.alpha_, linestyle=&quot;-&quot;, c=&quot;k&quot;, label=&quot;alphaCV&quot;, ) plt.ylabel(&quot;Regression Coefficients&quot;) ax.legend(X.columns, bbox_to_anchor=(1, 1)) plt.xlabel(&quot;alpha&quot;) plt.title( &quot;Regression Coefficients Progression for Lasso Paths&quot; ) . Text(0.5,1,&#39;Regression Coefficients Progression for Lasso Paths&#39;) . &#51116;&#44480;&#51201; &#53945;&#51669; &#51228;&#44144; . from yellowbrick.features import RFECV fig, ax = plt.subplots(figsize=(6, 4)) rfe = RFECV( ensemble.RandomForestClassifier( n_estimators=100 ), cv=5, ) rfe.fit(X, y) rfe.rfe_estimator_.ranking_ . array([1, 1, 1, 1, 1, 1, 3, 2]) . rfe.rfe_estimator_.n_features_ . 6 . rfe.poof() . 8.4 &#49345;&#54840; &#51221;&#48372;&#47049; . from sklearn import feature_selection mic = feature_selection.mutual_info_classif(X, y) fig, ax = plt.subplots(figsize=(10, 8)) ( pd.DataFrame( {&quot;feature&quot;: X.columns, &quot;vimp&quot;: mic} ) .set_index(&quot;feature&quot;) .plot.barh(ax=ax) ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5ed5e6b0d0&gt; .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter8/",
            "relUrl": "/chapter8/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "7장 데이터 전처리",
            "content": "X2 = pd.DataFrame( { &quot;a&quot;: range(5), &quot;b&quot;: [-100, -50, 0, 200, 1000], } ) X2 . a b . 0 0 | -100 | . 1 1 | -50 | . 2 2 | 0 | . 3 3 | 200 | . 4 4 | 1000 | . 7.1 &#54364;&#51456;&#54868; . from sklearn import preprocessing std = preprocessing.StandardScaler() std.fit_transform(X2) . array([[-1.41421356, -0.75995002], [-0.70710678, -0.63737744], [ 0. , -0.51480485], [ 0.70710678, -0.02451452], [ 1.41421356, 1.93664683]]) . std.scale_ . array([ 1.41421356, 407.92156109]) . std.mean_ . array([ 2., 210.]) . std.var_ . array([2.000e+00, 1.664e+05]) . X_std = (X2 - X2.mean()) / X2.std() X_std . a b . 0 -1.264911 | -0.679720 | . 1 -0.632456 | -0.570088 | . 2 0.000000 | -0.460455 | . 3 0.632456 | -0.021926 | . 4 1.264911 | 1.732190 | . X_std.mean() . a 4.440892e-17 b 0.000000e+00 dtype: float64 . X_std.std() . a 1.0 b 1.0 dtype: float64 . X3 = X2.copy() scale_vars(X3, mapper=None) #fastai function locally imported X3.std() . a 1.118034 b 1.118034 dtype: float64 . X3.mean() . a 0.000000e+00 b 4.440892e-17 dtype: float64 . 7.2 &#48276;&#50948; &#51312;&#51221; . from sklearn import preprocessing mms = preprocessing.MinMaxScaler() mms.fit(X2) mms.transform(X2) . array([[0. , 0. ], [0.25 , 0.04545455], [0.5 , 0.09090909], [0.75 , 0.27272727], [1. , 1. ]]) . (X2 - X2.min()) / (X2.max() - X2.min()) . a b . 0 0.00 | 0.000000 | . 1 0.25 | 0.045455 | . 2 0.50 | 0.090909 | . 3 0.75 | 0.272727 | . 4 1.00 | 1.000000 | . 7.3 &#45908;&#48120; &#48320;&#49688; . X_cat = pd.DataFrame( { &quot;name&quot;: [&quot;George&quot;, &quot;Paul&quot;], &quot;inst&quot;: [&quot;Bass&quot;, &quot;Guitar&quot;], } ) X_cat . name inst . 0 George | Bass | . 1 Paul | Guitar | . pd.get_dummies(X_cat, drop_first=True) . name_Paul inst_Guitar . 0 0 | 0 | . 1 1 | 1 | . import janitor as jn X_cat2 = pd.DataFrame( { &quot;A&quot;: [1, None, 3], &quot;names&quot;: [ &quot;Fred,George&quot;, &quot;George&quot;, &quot;John,Paul&quot;, ], } ) jn.expand_column(X_cat2, &quot;names&quot;, sep=&quot;,&quot;) . A names Fred George John Paul . 0 1.0 | Fred,George | 1 | 1 | 0 | 0 | . 1 NaN | George | 0 | 1 | 0 | 0 | . 2 3.0 | John,Paul | 0 | 0 | 1 | 1 | . 7.4 &#47112;&#51060;&#48660; &#51064;&#53076;&#45908; . from sklearn import preprocessing lab = preprocessing.LabelEncoder() lab.fit_transform(X_cat[&#39;inst&#39;]) . array([0, 1]) . lab.inverse_transform([1, 1, 0]) . array([&#39;Guitar&#39;, &#39;Guitar&#39;, &#39;Bass&#39;], dtype=object) . X_cat.name.astype( &quot;category&quot; ).cat.as_ordered().cat.codes + 1 . 0 1 1 2 dtype: int8 . 7.5 &#54532;&#47532;&#53248;&#49884; &#51064;&#53076;&#46377; . mapping = X_cat.name.value_counts() X_cat.name.map(mapping) . 0 1 1 1 Name: name, dtype: int64 . 7.6 &#47928;&#51088;&#50676;&#50640;&#49436; &#48276;&#51452; &#44032;&#51256;&#50724;&#44592; . df . pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest . 0 1 | 1 | Allen, Miss. Elisabeth Walton | female | 29.0000 | 0 | 0 | 24160 | 211.3375 | B5 | S | 2 | NaN | St Louis, MO | . 1 1 | 1 | Allison, Master. Hudson Trevor | male | 0.9167 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | 11 | NaN | Montreal, PQ / Chesterville, ON | . 2 1 | 0 | Allison, Miss. Helen Loraine | female | 2.0000 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | NaN | Montreal, PQ / Chesterville, ON | . 3 1 | 0 | Allison, Mr. Hudson Joshua Creighton | male | 30.0000 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | 135.0 | Montreal, PQ / Chesterville, ON | . 4 1 | 0 | Allison, Mrs. Hudson J C (Bessie Waldo Daniels) | female | 25.0000 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | NaN | Montreal, PQ / Chesterville, ON | . 5 1 | 1 | Anderson, Mr. Harry | male | 48.0000 | 0 | 0 | 19952 | 26.5500 | E12 | S | 3 | NaN | New York, NY | . 6 1 | 1 | Andrews, Miss. Kornelia Theodosia | female | 63.0000 | 1 | 0 | 13502 | 77.9583 | D7 | S | 10 | NaN | Hudson, NY | . 7 1 | 0 | Andrews, Mr. Thomas Jr | male | 39.0000 | 0 | 0 | 112050 | 0.0000 | A36 | S | NaN | NaN | Belfast, NI | . 8 1 | 1 | Appleton, Mrs. Edward Dale (Charlotte Lamson) | female | 53.0000 | 2 | 0 | 11769 | 51.4792 | C101 | S | D | NaN | Bayside, Queens, NY | . 9 1 | 0 | Artagaveytia, Mr. Ramon | male | 71.0000 | 0 | 0 | PC 17609 | 49.5042 | NaN | C | NaN | 22.0 | Montevideo, Uruguay | . 10 1 | 0 | Astor, Col. John Jacob | male | 47.0000 | 1 | 0 | PC 17757 | 227.5250 | C62 C64 | C | NaN | 124.0 | New York, NY | . 11 1 | 1 | Astor, Mrs. John Jacob (Madeleine Talmadge Force) | female | 18.0000 | 1 | 0 | PC 17757 | 227.5250 | C62 C64 | C | 4 | NaN | New York, NY | . 12 1 | 1 | Aubart, Mme. Leontine Pauline | female | 24.0000 | 0 | 0 | PC 17477 | 69.3000 | B35 | C | 9 | NaN | Paris, France | . 13 1 | 1 | Barber, Miss. Ellen &quot;Nellie&quot; | female | 26.0000 | 0 | 0 | 19877 | 78.8500 | NaN | S | 6 | NaN | NaN | . 14 1 | 1 | Barkworth, Mr. Algernon Henry Wilson | male | 80.0000 | 0 | 0 | 27042 | 30.0000 | A23 | S | B | NaN | Hessle, Yorks | . 15 1 | 0 | Baumann, Mr. John D | male | NaN | 0 | 0 | PC 17318 | 25.9250 | NaN | S | NaN | NaN | New York, NY | . 16 1 | 0 | Baxter, Mr. Quigg Edmond | male | 24.0000 | 0 | 1 | PC 17558 | 247.5208 | B58 B60 | C | NaN | NaN | Montreal, PQ | . 17 1 | 1 | Baxter, Mrs. James (Helene DeLaudeniere Chaput) | female | 50.0000 | 0 | 1 | PC 17558 | 247.5208 | B58 B60 | C | 6 | NaN | Montreal, PQ | . 18 1 | 1 | Bazzani, Miss. Albina | female | 32.0000 | 0 | 0 | 11813 | 76.2917 | D15 | C | 8 | NaN | NaN | . 19 1 | 0 | Beattie, Mr. Thomson | male | 36.0000 | 0 | 0 | 13050 | 75.2417 | C6 | C | A | NaN | Winnipeg, MN | . 20 1 | 1 | Beckwith, Mr. Richard Leonard | male | 37.0000 | 1 | 1 | 11751 | 52.5542 | D35 | S | 5 | NaN | New York, NY | . 21 1 | 1 | Beckwith, Mrs. Richard Leonard (Sallie Monypeny) | female | 47.0000 | 1 | 1 | 11751 | 52.5542 | D35 | S | 5 | NaN | New York, NY | . 22 1 | 1 | Behr, Mr. Karl Howell | male | 26.0000 | 0 | 0 | 111369 | 30.0000 | C148 | C | 5 | NaN | New York, NY | . 23 1 | 1 | Bidois, Miss. Rosalie | female | 42.0000 | 0 | 0 | PC 17757 | 227.5250 | NaN | C | 4 | NaN | NaN | . 24 1 | 1 | Bird, Miss. Ellen | female | 29.0000 | 0 | 0 | PC 17483 | 221.7792 | C97 | S | 8 | NaN | NaN | . 25 1 | 0 | Birnbaum, Mr. Jakob | male | 25.0000 | 0 | 0 | 13905 | 26.0000 | NaN | C | NaN | 148.0 | San Francisco, CA | . 26 1 | 1 | Bishop, Mr. Dickinson H | male | 25.0000 | 1 | 0 | 11967 | 91.0792 | B49 | C | 7 | NaN | Dowagiac, MI | . 27 1 | 1 | Bishop, Mrs. Dickinson H (Helen Walton) | female | 19.0000 | 1 | 0 | 11967 | 91.0792 | B49 | C | 7 | NaN | Dowagiac, MI | . 28 1 | 1 | Bissette, Miss. Amelia | female | 35.0000 | 0 | 0 | PC 17760 | 135.6333 | C99 | S | 8 | NaN | NaN | . 29 1 | 1 | Bjornstrom-Steffansson, Mr. Mauritz Hakan | male | 28.0000 | 0 | 0 | 110564 | 26.5500 | C52 | S | D | NaN | Stockholm, Sweden / Washington, DC | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1279 3 | 0 | Vestrom, Miss. Hulda Amanda Adolfina | female | 14.0000 | 0 | 0 | 350406 | 7.8542 | NaN | S | NaN | NaN | NaN | . 1280 3 | 0 | Vovk, Mr. Janko | male | 22.0000 | 0 | 0 | 349252 | 7.8958 | NaN | S | NaN | NaN | NaN | . 1281 3 | 0 | Waelens, Mr. Achille | male | 22.0000 | 0 | 0 | 345767 | 9.0000 | NaN | S | NaN | NaN | Antwerp, Belgium / Stanton, OH | . 1282 3 | 0 | Ware, Mr. Frederick | male | NaN | 0 | 0 | 359309 | 8.0500 | NaN | S | NaN | NaN | NaN | . 1283 3 | 0 | Warren, Mr. Charles William | male | NaN | 0 | 0 | C.A. 49867 | 7.5500 | NaN | S | NaN | NaN | NaN | . 1284 3 | 0 | Webber, Mr. James | male | NaN | 0 | 0 | SOTON/OQ 3101316 | 8.0500 | NaN | S | NaN | NaN | NaN | . 1285 3 | 0 | Wenzel, Mr. Linhart | male | 32.5000 | 0 | 0 | 345775 | 9.5000 | NaN | S | NaN | 298.0 | NaN | . 1286 3 | 1 | Whabee, Mrs. George Joseph (Shawneene Abi-Saab) | female | 38.0000 | 0 | 0 | 2688 | 7.2292 | NaN | C | C | NaN | NaN | . 1287 3 | 0 | Widegren, Mr. Carl/Charles Peter | male | 51.0000 | 0 | 0 | 347064 | 7.7500 | NaN | S | NaN | NaN | NaN | . 1288 3 | 0 | Wiklund, Mr. Jakob Alfred | male | 18.0000 | 1 | 0 | 3101267 | 6.4958 | NaN | S | NaN | 314.0 | NaN | . 1289 3 | 0 | Wiklund, Mr. Karl Johan | male | 21.0000 | 1 | 0 | 3101266 | 6.4958 | NaN | S | NaN | NaN | NaN | . 1290 3 | 1 | Wilkes, Mrs. James (Ellen Needs) | female | 47.0000 | 1 | 0 | 363272 | 7.0000 | NaN | S | NaN | NaN | NaN | . 1291 3 | 0 | Willer, Mr. Aaron (&quot;Abi Weller&quot;) | male | NaN | 0 | 0 | 3410 | 8.7125 | NaN | S | NaN | NaN | NaN | . 1292 3 | 0 | Willey, Mr. Edward | male | NaN | 0 | 0 | S.O./P.P. 751 | 7.5500 | NaN | S | NaN | NaN | NaN | . 1293 3 | 0 | Williams, Mr. Howard Hugh &quot;Harry&quot; | male | NaN | 0 | 0 | A/5 2466 | 8.0500 | NaN | S | NaN | NaN | NaN | . 1294 3 | 0 | Williams, Mr. Leslie | male | 28.5000 | 0 | 0 | 54636 | 16.1000 | NaN | S | NaN | 14.0 | NaN | . 1295 3 | 0 | Windelov, Mr. Einar | male | 21.0000 | 0 | 0 | SOTON/OQ 3101317 | 7.2500 | NaN | S | NaN | NaN | NaN | . 1296 3 | 0 | Wirz, Mr. Albert | male | 27.0000 | 0 | 0 | 315154 | 8.6625 | NaN | S | NaN | 131.0 | NaN | . 1297 3 | 0 | Wiseman, Mr. Phillippe | male | NaN | 0 | 0 | A/4. 34244 | 7.2500 | NaN | S | NaN | NaN | NaN | . 1298 3 | 0 | Wittevrongel, Mr. Camille | male | 36.0000 | 0 | 0 | 345771 | 9.5000 | NaN | S | NaN | NaN | NaN | . 1299 3 | 0 | Yasbeck, Mr. Antoni | male | 27.0000 | 1 | 0 | 2659 | 14.4542 | NaN | C | C | NaN | NaN | . 1300 3 | 1 | Yasbeck, Mrs. Antoni (Selini Alexander) | female | 15.0000 | 1 | 0 | 2659 | 14.4542 | NaN | C | NaN | NaN | NaN | . 1301 3 | 0 | Youseff, Mr. Gerious | male | 45.5000 | 0 | 0 | 2628 | 7.2250 | NaN | C | NaN | 312.0 | NaN | . 1302 3 | 0 | Yousif, Mr. Wazli | male | NaN | 0 | 0 | 2647 | 7.2250 | NaN | C | NaN | NaN | NaN | . 1303 3 | 0 | Yousseff, Mr. Gerious | male | NaN | 0 | 0 | 2627 | 14.4583 | NaN | C | NaN | NaN | NaN | . 1304 3 | 0 | Zabour, Miss. Hileni | female | 14.5000 | 1 | 0 | 2665 | 14.4542 | NaN | C | NaN | 328.0 | NaN | . 1305 3 | 0 | Zabour, Miss. Thamine | female | NaN | 1 | 0 | 2665 | 14.4542 | NaN | C | NaN | NaN | NaN | . 1306 3 | 0 | Zakarian, Mr. Mapriededer | male | 26.5000 | 0 | 0 | 2656 | 7.2250 | NaN | C | NaN | 304.0 | NaN | . 1307 3 | 0 | Zakarian, Mr. Ortin | male | 27.0000 | 0 | 0 | 2670 | 7.2250 | NaN | C | NaN | NaN | NaN | . 1308 3 | 0 | Zimmerman, Mr. Leo | male | 29.0000 | 0 | 0 | 315082 | 7.8750 | NaN | S | NaN | NaN | NaN | . 1309 rows × 14 columns . from collections import Counter c = Counter() def triples(val): for i in range(len(val)): c[val[i : i + 3]] += 1 df.name.apply(triples) c.most_common(10) . [(&#39;, M&#39;, 1282), (&#39; Mr&#39;, 954), (&#39;r. &#39;, 830), (&#39;Mr.&#39;, 757), (&#39;s. &#39;, 460), (&#39;n, &#39;, 320), (&#39; Mi&#39;, 283), (&#39;iss&#39;, 261), (&#39;ss.&#39;, 261), (&#39;Mis&#39;, 260)] . df.name.str.extract( &quot;([A-Za-z]+) .&quot;, expand=False ).head() . 0 Miss 1 Master 2 Miss 3 Mr 4 Mrs Name: name, dtype: object . df.name.str.extract( &quot;([A-Za-z]+) .&quot;, expand=False ).value_counts() . Mr 757 Miss 260 Mrs 197 Master 61 Dr 8 Rev 8 Col 4 Ms 2 Mlle 2 Major 2 Mme 1 Dona 1 Don 1 Countess 1 Sir 1 Lady 1 Jonkheer 1 Capt 1 Name: name, dtype: int64 . 7.7 &#44536; &#48150;&#51032; &#48276;&#51452;&#54805; &#51064;&#53076;&#46377; . import category_encoders as ce he = ce.HashingEncoder(verbose=1) he.fit_transform(X_cat) . col_0 col_1 col_2 col_3 col_4 col_5 col_6 col_7 . 0 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 1 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | . size_df = pd.DataFrame( { &quot;name&quot;: [&quot;Fred&quot;, &quot;John&quot;, &quot;Matt&quot;], &quot;size&quot;: [&quot;small&quot;, &quot;med&quot;, &quot;xxl&quot;], } ) ore = ce.OrdinalEncoder( mapping=[ { &quot;col&quot;: &quot;size&quot;, &quot;mapping&quot;: { &quot;small&quot;: 1, &quot;med&quot;: 2, &quot;lg&quot;: 3 } } ] ) ore.fit_transform(size_df) . name size . 0 Fred | 1.0 | . 1 John | 2.0 | . 2 Matt | -1.0 | . def get_title(df): return df.name.str.extract( &quot;([A-Za-z]+) .&quot;, expand=False ) te = ce.TargetEncoder(cols=&quot;Title&quot;) te.fit_transform( df.assign(Title=get_title), df.survived)[&quot;Title&quot;].head() . 0 0.676923 1 0.508197 2 0.676923 3 0.162483 4 0.786802 Name: Title, dtype: float64 . 7.8 &#45216;&#51676;&#54805; &#45936;&#51060;&#53552;&#51032; &#53945;&#51669; &#44277;&#54617; . import numpy as np dates = pd.DataFrame( { &quot;A&quot;: pd.to_datetime( [&quot;9/17/2001&quot;, &quot;Jan 1, 2002&quot;] ) } ) add_datepart(dates, &quot;A&quot;) dates.T . 0 1 . AYear 2001 | 2002 | . AMonth 9 | 1 | . AWeek 38 | 1 | . ADay 17 | 1 | . ADayofweek 0 | 1 | . ADayofyear 260 | 1 | . AIs_month_end False | False | . AIs_month_start False | True | . AIs_quarter_end False | False | . AIs_quarter_start False | True | . AIs_year_end False | False | . AIs_year_start False | True | . AElapsed 1000684800 | 1009843200 | . 7.9 col_na &#53945;&#51669;&#51032; &#52628;&#44032; . from pandas.api.types import is_numeric_dtype def fix_missing(df, col, name, na_dict): if is_numeric_dtype(col): if pd.isnull(col).sum() or (name in na_dict): df[name + &quot;_na&quot;] = pd.isnull(col) filler = ( na_dict[name] if name in na_dict else col.median() ) df[name] = col.fillna(filler) na_dict[name] = filler return na_dict data = pd.DataFrame({&quot;A&quot;: [0, None, 5, 100]}) fix_missing(data, data.A, &quot;A&quot;, {}) . {&#39;A&#39;: 5.0} . from pandas.api.types import is_numeric_dtype def fix_missing(df, col, name, na_dict): if is_numeric_dtype(col): if pd.isnull(col).sum() or (name in na_dict): df[name + &quot;_na&quot;] = pd.isnull(col) filler = ( na_dict[name] if name in na_dict else col.median() ) df[name] = col.fillna(filler) na_dict[name] = filler return na_dict data = pd.DataFrame({&quot;A&quot;: [0, None, 5, 100]}) fix_missing(data, data.A, &quot;A&quot;, {}) . {&#39;A&#39;: 5.0} . data . A A_na . 0 0.0 | False | . 1 5.0 | True | . 2 5.0 | False | . 3 100.0 | False | . data = pd.DataFrame({&quot;A&quot;: [0, None, 5, 100]}) data[&quot;A_na&quot;] = data.A.isnull() data[&quot;A&quot;] = data.A.fillna(data.A.median()) . 7.10 &#49688;&#46041;&#51201; &#53945;&#51669; &#44277;&#54617; . agg = ( df.groupby(&quot;cabin&quot;) .agg(&quot;min,max,mean,sum&quot;.split(&quot;,&quot;)) .reset_index() ) agg.columns = [ &quot;_&quot;.join(c).strip(&quot;_&quot;) for c in agg.columns.values ] agg_df = df.merge(agg, on=&quot;cabin&quot;) agg_df . pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest pclass_min pclass_max pclass_mean pclass_sum survived_min survived_max survived_mean survived_sum age_min age_max age_mean age_sum sibsp_min sibsp_max sibsp_mean sibsp_sum parch_min parch_max parch_mean parch_sum fare_min fare_max fare_mean fare_sum body_min body_max body_mean body_sum . 0 1 | 1 | Allen, Miss. Elisabeth Walton | female | 29.0000 | 0 | 0 | 24160 | 211.3375 | B5 | S | 2 | NaN | St Louis, MO | 1 | 1 | 1 | 2 | 1 | 1 | 1.000000 | 2 | 15.0000 | 29.0 | 22.000000 | 44.0000 | 0 | 0 | 0.000000 | 0 | 0 | 1 | 0.5 | 1 | 211.3375 | 211.3375 | 211.337500 | 422.6750 | NaN | NaN | NaN | 0.0 | . 1 1 | 1 | Madill, Miss. Georgette Alexandra | female | 15.0000 | 0 | 1 | 24160 | 211.3375 | B5 | S | 2 | NaN | St Louis, MO | 1 | 1 | 1 | 2 | 1 | 1 | 1.000000 | 2 | 15.0000 | 29.0 | 22.000000 | 44.0000 | 0 | 0 | 0.000000 | 0 | 0 | 1 | 0.5 | 1 | 211.3375 | 211.3375 | 211.337500 | 422.6750 | NaN | NaN | NaN | 0.0 | . 2 1 | 1 | Allison, Master. Hudson Trevor | male | 0.9167 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | 11 | NaN | Montreal, PQ / Chesterville, ON | 1 | 1 | 1 | 4 | 0 | 1 | 0.250000 | 1 | 0.9167 | 30.0 | 14.479175 | 57.9167 | 1 | 1 | 1.000000 | 4 | 2 | 2 | 2.0 | 8 | 151.5500 | 151.5500 | 151.550000 | 606.2000 | 135.0 | 135.0 | 135.0 | 135.0 | . 3 1 | 0 | Allison, Miss. Helen Loraine | female | 2.0000 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | NaN | Montreal, PQ / Chesterville, ON | 1 | 1 | 1 | 4 | 0 | 1 | 0.250000 | 1 | 0.9167 | 30.0 | 14.479175 | 57.9167 | 1 | 1 | 1.000000 | 4 | 2 | 2 | 2.0 | 8 | 151.5500 | 151.5500 | 151.550000 | 606.2000 | 135.0 | 135.0 | 135.0 | 135.0 | . 4 1 | 0 | Allison, Mr. Hudson Joshua Creighton | male | 30.0000 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | 135.0 | Montreal, PQ / Chesterville, ON | 1 | 1 | 1 | 4 | 0 | 1 | 0.250000 | 1 | 0.9167 | 30.0 | 14.479175 | 57.9167 | 1 | 1 | 1.000000 | 4 | 2 | 2 | 2.0 | 8 | 151.5500 | 151.5500 | 151.550000 | 606.2000 | 135.0 | 135.0 | 135.0 | 135.0 | . 5 1 | 0 | Allison, Mrs. Hudson J C (Bessie Waldo Daniels) | female | 25.0000 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | NaN | Montreal, PQ / Chesterville, ON | 1 | 1 | 1 | 4 | 0 | 1 | 0.250000 | 1 | 0.9167 | 30.0 | 14.479175 | 57.9167 | 1 | 1 | 1.000000 | 4 | 2 | 2 | 2.0 | 8 | 151.5500 | 151.5500 | 151.550000 | 606.2000 | 135.0 | 135.0 | 135.0 | 135.0 | . 6 1 | 1 | Anderson, Mr. Harry | male | 48.0000 | 0 | 0 | 19952 | 26.5500 | E12 | S | 3 | NaN | New York, NY | 1 | 1 | 1 | 1 | 1 | 1 | 1.000000 | 1 | 48.0000 | 48.0 | 48.000000 | 48.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 26.5500 | 26.5500 | 26.550000 | 26.5500 | NaN | NaN | NaN | 0.0 | . 7 1 | 1 | Andrews, Miss. Kornelia Theodosia | female | 63.0000 | 1 | 0 | 13502 | 77.9583 | D7 | S | 10 | NaN | Hudson, NY | 1 | 1 | 1 | 1 | 1 | 1 | 1.000000 | 1 | 63.0000 | 63.0 | 63.000000 | 63.0000 | 1 | 1 | 1.000000 | 1 | 0 | 0 | 0.0 | 0 | 77.9583 | 77.9583 | 77.958300 | 77.9583 | NaN | NaN | NaN | 0.0 | . 8 1 | 0 | Andrews, Mr. Thomas Jr | male | 39.0000 | 0 | 0 | 112050 | 0.0000 | A36 | S | NaN | NaN | Belfast, NI | 1 | 1 | 1 | 1 | 0 | 0 | 0.000000 | 0 | 39.0000 | 39.0 | 39.000000 | 39.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 0.0000 | 0.0000 | 0.000000 | 0.0000 | NaN | NaN | NaN | 0.0 | . 9 1 | 1 | Appleton, Mrs. Edward Dale (Charlotte Lamson) | female | 53.0000 | 2 | 0 | 11769 | 51.4792 | C101 | S | D | NaN | Bayside, Queens, NY | 1 | 1 | 1 | 3 | 1 | 1 | 1.000000 | 3 | 53.0000 | 59.0 | 55.666667 | 167.0000 | 2 | 2 | 2.000000 | 6 | 0 | 0 | 0.0 | 0 | 25.7000 | 51.4792 | 42.886133 | 128.6584 | NaN | NaN | NaN | 0.0 | . 10 1 | 1 | Brown, Mrs. John Murray (Caroline Lane Lamson) | female | 59.0000 | 2 | 0 | 11769 | 51.4792 | C101 | S | D | NaN | Belmont, MA | 1 | 1 | 1 | 3 | 1 | 1 | 1.000000 | 3 | 53.0000 | 59.0 | 55.666667 | 167.0000 | 2 | 2 | 2.000000 | 6 | 0 | 0 | 0.0 | 0 | 25.7000 | 51.4792 | 42.886133 | 128.6584 | NaN | NaN | NaN | 0.0 | . 11 1 | 1 | Cornell, Mrs. Robert Clifford (Malvina Helen L... | female | 55.0000 | 2 | 0 | 11770 | 25.7000 | C101 | S | 2 | NaN | New York, NY | 1 | 1 | 1 | 3 | 1 | 1 | 1.000000 | 3 | 53.0000 | 59.0 | 55.666667 | 167.0000 | 2 | 2 | 2.000000 | 6 | 0 | 0 | 0.0 | 0 | 25.7000 | 51.4792 | 42.886133 | 128.6584 | NaN | NaN | NaN | 0.0 | . 12 1 | 0 | Astor, Col. John Jacob | male | 47.0000 | 1 | 0 | PC 17757 | 227.5250 | C62 C64 | C | NaN | 124.0 | New York, NY | 1 | 1 | 1 | 2 | 0 | 1 | 0.500000 | 1 | 18.0000 | 47.0 | 32.500000 | 65.0000 | 1 | 1 | 1.000000 | 2 | 0 | 0 | 0.0 | 0 | 227.5250 | 227.5250 | 227.525000 | 455.0500 | 124.0 | 124.0 | 124.0 | 124.0 | . 13 1 | 1 | Astor, Mrs. John Jacob (Madeleine Talmadge Force) | female | 18.0000 | 1 | 0 | PC 17757 | 227.5250 | C62 C64 | C | 4 | NaN | New York, NY | 1 | 1 | 1 | 2 | 0 | 1 | 0.500000 | 1 | 18.0000 | 47.0 | 32.500000 | 65.0000 | 1 | 1 | 1.000000 | 2 | 0 | 0 | 0.0 | 0 | 227.5250 | 227.5250 | 227.525000 | 455.0500 | 124.0 | 124.0 | 124.0 | 124.0 | . 14 1 | 1 | Aubart, Mme. Leontine Pauline | female | 24.0000 | 0 | 0 | PC 17477 | 69.3000 | B35 | C | 9 | NaN | Paris, France | 1 | 1 | 1 | 2 | 1 | 1 | 1.000000 | 2 | 24.0000 | 24.0 | 24.000000 | 48.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 69.3000 | 69.3000 | 69.300000 | 138.6000 | NaN | NaN | NaN | 0.0 | . 15 1 | 1 | Sagesser, Mlle. Emma | female | 24.0000 | 0 | 0 | PC 17477 | 69.3000 | B35 | C | 9 | NaN | NaN | 1 | 1 | 1 | 2 | 1 | 1 | 1.000000 | 2 | 24.0000 | 24.0 | 24.000000 | 48.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 69.3000 | 69.3000 | 69.300000 | 138.6000 | NaN | NaN | NaN | 0.0 | . 16 1 | 1 | Barkworth, Mr. Algernon Henry Wilson | male | 80.0000 | 0 | 0 | 27042 | 30.0000 | A23 | S | B | NaN | Hessle, Yorks | 1 | 1 | 1 | 1 | 1 | 1 | 1.000000 | 1 | 80.0000 | 80.0 | 80.000000 | 80.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 30.0000 | 30.0000 | 30.000000 | 30.0000 | NaN | NaN | NaN | 0.0 | . 17 1 | 0 | Baxter, Mr. Quigg Edmond | male | 24.0000 | 0 | 1 | PC 17558 | 247.5208 | B58 B60 | C | NaN | NaN | Montreal, PQ | 1 | 1 | 1 | 3 | 0 | 1 | 0.666667 | 2 | 24.0000 | 50.0 | 33.666667 | 101.0000 | 0 | 1 | 0.333333 | 1 | 1 | 1 | 1.0 | 3 | 247.5208 | 247.5208 | 247.520800 | 742.5624 | NaN | NaN | NaN | 0.0 | . 18 1 | 1 | Baxter, Mrs. James (Helene DeLaudeniere Chaput) | female | 50.0000 | 0 | 1 | PC 17558 | 247.5208 | B58 B60 | C | 6 | NaN | Montreal, PQ | 1 | 1 | 1 | 3 | 0 | 1 | 0.666667 | 2 | 24.0000 | 50.0 | 33.666667 | 101.0000 | 0 | 1 | 0.333333 | 1 | 1 | 1 | 1.0 | 3 | 247.5208 | 247.5208 | 247.520800 | 742.5624 | NaN | NaN | NaN | 0.0 | . 19 1 | 1 | Douglas, Mrs. Frederick Charles (Mary Helene B... | female | 27.0000 | 1 | 1 | PC 17558 | 247.5208 | B58 B60 | C | 6 | NaN | Montreal, PQ | 1 | 1 | 1 | 3 | 0 | 1 | 0.666667 | 2 | 24.0000 | 50.0 | 33.666667 | 101.0000 | 0 | 1 | 0.333333 | 1 | 1 | 1 | 1.0 | 3 | 247.5208 | 247.5208 | 247.520800 | 742.5624 | NaN | NaN | NaN | 0.0 | . 20 1 | 1 | Bazzani, Miss. Albina | female | 32.0000 | 0 | 0 | 11813 | 76.2917 | D15 | C | 8 | NaN | NaN | 1 | 1 | 1 | 2 | 1 | 1 | 1.000000 | 2 | 32.0000 | 60.0 | 46.000000 | 92.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 76.2917 | 76.2917 | 76.291700 | 152.5834 | NaN | NaN | NaN | 0.0 | . 21 1 | 1 | Bucknell, Mrs. William Robert (Emma Eliza Ward) | female | 60.0000 | 0 | 0 | 11813 | 76.2917 | D15 | C | 8 | NaN | Philadelphia, PA | 1 | 1 | 1 | 2 | 1 | 1 | 1.000000 | 2 | 32.0000 | 60.0 | 46.000000 | 92.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 76.2917 | 76.2917 | 76.291700 | 152.5834 | NaN | NaN | NaN | 0.0 | . 22 1 | 0 | Beattie, Mr. Thomson | male | 36.0000 | 0 | 0 | 13050 | 75.2417 | C6 | C | A | NaN | Winnipeg, MN | 1 | 1 | 1 | 2 | 0 | 0 | 0.000000 | 0 | 36.0000 | 46.0 | 41.000000 | 82.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 75.2417 | 75.2417 | 75.241700 | 150.4834 | 292.0 | 292.0 | 292.0 | 292.0 | . 23 1 | 0 | McCaffry, Mr. Thomas Francis | male | 46.0000 | 0 | 0 | 13050 | 75.2417 | C6 | C | NaN | 292.0 | Vancouver, BC | 1 | 1 | 1 | 2 | 0 | 0 | 0.000000 | 0 | 36.0000 | 46.0 | 41.000000 | 82.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 75.2417 | 75.2417 | 75.241700 | 150.4834 | 292.0 | 292.0 | 292.0 | 292.0 | . 24 1 | 1 | Beckwith, Mr. Richard Leonard | male | 37.0000 | 1 | 1 | 11751 | 52.5542 | D35 | S | 5 | NaN | New York, NY | 1 | 1 | 1 | 2 | 1 | 1 | 1.000000 | 2 | 37.0000 | 47.0 | 42.000000 | 84.0000 | 1 | 1 | 1.000000 | 2 | 1 | 1 | 1.0 | 2 | 52.5542 | 52.5542 | 52.554200 | 105.1084 | NaN | NaN | NaN | 0.0 | . 25 1 | 1 | Beckwith, Mrs. Richard Leonard (Sallie Monypeny) | female | 47.0000 | 1 | 1 | 11751 | 52.5542 | D35 | S | 5 | NaN | New York, NY | 1 | 1 | 1 | 2 | 1 | 1 | 1.000000 | 2 | 37.0000 | 47.0 | 42.000000 | 84.0000 | 1 | 1 | 1.000000 | 2 | 1 | 1 | 1.0 | 2 | 52.5542 | 52.5542 | 52.554200 | 105.1084 | NaN | NaN | NaN | 0.0 | . 26 1 | 1 | Behr, Mr. Karl Howell | male | 26.0000 | 0 | 0 | 111369 | 30.0000 | C148 | C | 5 | NaN | New York, NY | 1 | 1 | 1 | 1 | 1 | 1 | 1.000000 | 1 | 26.0000 | 26.0 | 26.000000 | 26.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 30.0000 | 30.0000 | 30.000000 | 30.0000 | NaN | NaN | NaN | 0.0 | . 27 1 | 1 | Bird, Miss. Ellen | female | 29.0000 | 0 | 0 | PC 17483 | 221.7792 | C97 | S | 8 | NaN | NaN | 1 | 1 | 1 | 1 | 1 | 1 | 1.000000 | 1 | 29.0000 | 29.0 | 29.000000 | 29.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 221.7792 | 221.7792 | 221.779200 | 221.7792 | NaN | NaN | NaN | 0.0 | . 28 1 | 1 | Bishop, Mr. Dickinson H | male | 25.0000 | 1 | 0 | 11967 | 91.0792 | B49 | C | 7 | NaN | Dowagiac, MI | 1 | 1 | 1 | 2 | 1 | 1 | 1.000000 | 2 | 19.0000 | 25.0 | 22.000000 | 44.0000 | 1 | 1 | 1.000000 | 2 | 0 | 0 | 0.0 | 0 | 91.0792 | 91.0792 | 91.079200 | 182.1584 | NaN | NaN | NaN | 0.0 | . 29 1 | 1 | Bishop, Mrs. Dickinson H (Helen Walton) | female | 19.0000 | 1 | 0 | 11967 | 91.0792 | B49 | C | 7 | NaN | Dowagiac, MI | 1 | 1 | 1 | 2 | 1 | 1 | 1.000000 | 2 | 19.0000 | 25.0 | 22.000000 | 44.0000 | 1 | 1 | 1.000000 | 2 | 0 | 0 | 0.0 | 0 | 91.0792 | 91.0792 | 91.079200 | 182.1584 | NaN | NaN | NaN | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 265 2 | 1 | Brown, Miss. Amelia &quot;Mildred&quot; | female | 24.0000 | 0 | 0 | 248733 | 13.0000 | F33 | S | 11 | NaN | London / Montreal, PQ | 2 | 2 | 2 | 8 | 1 | 1 | 1.000000 | 4 | 22.0000 | 34.0 | 27.250000 | 109.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 10.5000 | 13.0000 | 11.125000 | 44.5000 | NaN | NaN | NaN | 0.0 | . 266 2 | 1 | Cook, Mrs. (Selena Rogers) | female | 22.0000 | 0 | 0 | W./C. 14266 | 10.5000 | F33 | S | 14 | NaN | Pennsylvania | 2 | 2 | 2 | 8 | 1 | 1 | 1.000000 | 4 | 22.0000 | 34.0 | 27.250000 | 109.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 10.5000 | 13.0000 | 11.125000 | 44.5000 | NaN | NaN | NaN | 0.0 | . 267 2 | 1 | Lemore, Mrs. (Amelia Milley) | female | 34.0000 | 0 | 0 | C.A. 34260 | 10.5000 | F33 | S | 14 | NaN | Chicago, IL | 2 | 2 | 2 | 8 | 1 | 1 | 1.000000 | 4 | 22.0000 | 34.0 | 27.250000 | 109.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 10.5000 | 13.0000 | 11.125000 | 44.5000 | NaN | NaN | NaN | 0.0 | . 268 2 | 1 | Nye, Mrs. (Elizabeth Ramell) | female | 29.0000 | 0 | 0 | C.A. 29395 | 10.5000 | F33 | S | 11 | NaN | Folkstone, Kent / New York, NY | 2 | 2 | 2 | 8 | 1 | 1 | 1.000000 | 4 | 22.0000 | 34.0 | 27.250000 | 109.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 10.5000 | 13.0000 | 11.125000 | 44.5000 | NaN | NaN | NaN | 0.0 | . 269 2 | 1 | Keane, Miss. Nora A | female | NaN | 0 | 0 | 226593 | 12.3500 | E101 | Q | 10 | NaN | Harrisburg, PA | 2 | 2 | 2 | 6 | 1 | 1 | 1.000000 | 3 | 27.0000 | 32.5 | 29.750000 | 59.5000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 10.5000 | 13.0000 | 11.950000 | 35.8500 | NaN | NaN | NaN | 0.0 | . 270 2 | 1 | Troutt, Miss. Edwina Celia &quot;Winnie&quot; | female | 27.0000 | 0 | 0 | 34218 | 10.5000 | E101 | S | 16 | NaN | Bath, England / Massachusetts | 2 | 2 | 2 | 6 | 1 | 1 | 1.000000 | 3 | 27.0000 | 32.5 | 29.750000 | 59.5000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 10.5000 | 13.0000 | 11.950000 | 35.8500 | NaN | NaN | NaN | 0.0 | . 271 2 | 1 | Webber, Miss. Susan | female | 32.5000 | 0 | 0 | 27267 | 13.0000 | E101 | S | 12 | NaN | England / Hartford, CT | 2 | 2 | 2 | 6 | 1 | 1 | 1.000000 | 3 | 27.0000 | 32.5 | 29.750000 | 59.5000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 10.5000 | 13.0000 | 11.950000 | 35.8500 | NaN | NaN | NaN | 0.0 | . 272 2 | 0 | Mack, Mrs. (Mary) | female | 57.0000 | 0 | 0 | S.O./P.P. 3 | 10.5000 | E77 | S | NaN | 52.0 | Southampton / New York, NY | 2 | 2 | 2 | 2 | 0 | 0 | 0.000000 | 0 | 57.0000 | 57.0 | 57.000000 | 57.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 10.5000 | 10.5000 | 10.500000 | 10.5000 | 52.0 | 52.0 | 52.0 | 52.0 | . 273 2 | 1 | Navratil, Master. Edmond Roger | male | 2.0000 | 1 | 1 | 230080 | 26.0000 | F2 | S | D | NaN | Nice, France | 2 | 2 | 2 | 8 | 0 | 1 | 0.500000 | 2 | 2.0000 | 36.5 | 16.875000 | 67.5000 | 0 | 1 | 0.500000 | 2 | 0 | 2 | 1.0 | 4 | 13.0000 | 26.0000 | 22.750000 | 91.0000 | 15.0 | 15.0 | 15.0 | 15.0 | . 274 2 | 1 | Navratil, Master. Michel M | male | 3.0000 | 1 | 1 | 230080 | 26.0000 | F2 | S | D | NaN | Nice, France | 2 | 2 | 2 | 8 | 0 | 1 | 0.500000 | 2 | 2.0000 | 36.5 | 16.875000 | 67.5000 | 0 | 1 | 0.500000 | 2 | 0 | 2 | 1.0 | 4 | 13.0000 | 26.0000 | 22.750000 | 91.0000 | 15.0 | 15.0 | 15.0 | 15.0 | . 275 2 | 0 | Navratil, Mr. Michel (&quot;Louis M Hoffman&quot;) | male | 36.5000 | 0 | 2 | 230080 | 26.0000 | F2 | S | NaN | 15.0 | Nice, France | 2 | 2 | 2 | 8 | 0 | 1 | 0.500000 | 2 | 2.0000 | 36.5 | 16.875000 | 67.5000 | 0 | 1 | 0.500000 | 2 | 0 | 2 | 1.0 | 4 | 13.0000 | 26.0000 | 22.750000 | 91.0000 | 15.0 | 15.0 | 15.0 | 15.0 | . 276 2 | 0 | Nesson, Mr. Israel | male | 26.0000 | 0 | 0 | 244368 | 13.0000 | F2 | S | NaN | NaN | Boston, MA | 2 | 2 | 2 | 8 | 0 | 1 | 0.500000 | 2 | 2.0000 | 36.5 | 16.875000 | 67.5000 | 0 | 1 | 0.500000 | 2 | 0 | 2 | 1.0 | 4 | 13.0000 | 26.0000 | 22.750000 | 91.0000 | 15.0 | 15.0 | 15.0 | 15.0 | . 277 2 | 1 | Nourney, Mr. Alfred (&quot;Baron von Drachstedt&quot;) | male | 20.0000 | 0 | 0 | SC/PARIS 2166 | 13.8625 | D38 | C | 7 | NaN | Cologne, Germany | 2 | 2 | 2 | 2 | 1 | 1 | 1.000000 | 1 | 20.0000 | 20.0 | 20.000000 | 20.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 13.8625 | 13.8625 | 13.862500 | 13.8625 | NaN | NaN | NaN | 0.0 | . 278 2 | 0 | Swane, Mr. George | male | 18.5000 | 0 | 0 | 248734 | 13.0000 | F | S | NaN | 294.0 | NaN | 2 | 2 | 2 | 2 | 0 | 0 | 0.000000 | 0 | 18.5000 | 18.5 | 18.500000 | 18.5000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 13.0000 | 13.0000 | 13.000000 | 13.0000 | 294.0 | 294.0 | 294.0 | 294.0 | . 279 3 | 1 | Abelseth, Mr. Olaus Jorgensen | male | 25.0000 | 0 | 0 | 348122 | 7.6500 | F G63 | S | A | NaN | Perkins County, SD | 3 | 3 | 3 | 6 | 0 | 1 | 0.500000 | 1 | 25.0000 | 42.0 | 33.500000 | 67.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 7.6500 | 7.6500 | 7.650000 | 15.3000 | 120.0 | 120.0 | 120.0 | 120.0 | . 280 3 | 0 | Humblen, Mr. Adolf Mathias Nicolai Olsen | male | 42.0000 | 0 | 0 | 348121 | 7.6500 | F G63 | S | NaN | 120.0 | NaN | 3 | 3 | 3 | 6 | 0 | 1 | 0.500000 | 1 | 25.0000 | 42.0 | 33.500000 | 67.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 7.6500 | 7.6500 | 7.650000 | 15.3000 | 120.0 | 120.0 | 120.0 | 120.0 | . 281 3 | 1 | Krekorian, Mr. Neshan | male | 25.0000 | 0 | 0 | 2654 | 7.2292 | F E57 | C | 10 | NaN | NaN | 3 | 3 | 3 | 3 | 1 | 1 | 1.000000 | 1 | 25.0000 | 25.0 | 25.000000 | 25.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 7.2292 | 7.2292 | 7.229200 | 7.2292 | NaN | NaN | NaN | 0.0 | . 282 3 | 0 | Mardirosian, Mr. Sarkis | male | NaN | 0 | 0 | 2655 | 7.2292 | F E46 | C | NaN | NaN | NaN | 3 | 3 | 3 | 3 | 0 | 0 | 0.000000 | 0 | NaN | NaN | NaN | 0.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 7.2292 | 7.2292 | 7.229200 | 7.2292 | NaN | NaN | NaN | 0.0 | . 283 3 | 0 | Moen, Mr. Sigurd Hansen | male | 25.0000 | 0 | 0 | 348123 | 7.6500 | F G73 | S | NaN | 309.0 | NaN | 3 | 3 | 3 | 6 | 0 | 0 | 0.000000 | 0 | 19.0000 | 25.0 | 22.000000 | 44.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 7.6500 | 7.6500 | 7.650000 | 15.3000 | 309.0 | 309.0 | 309.0 | 309.0 | . 284 3 | 0 | Soholt, Mr. Peter Andreas Lauritz Andersen | male | 19.0000 | 0 | 0 | 348124 | 7.6500 | F G73 | S | NaN | NaN | NaN | 3 | 3 | 3 | 6 | 0 | 0 | 0.000000 | 0 | 19.0000 | 25.0 | 22.000000 | 44.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 7.6500 | 7.6500 | 7.650000 | 15.3000 | 309.0 | 309.0 | 309.0 | 309.0 | . 285 3 | 1 | Moor, Master. Meier | male | 6.0000 | 0 | 1 | 392096 | 12.4750 | E121 | S | 14 | NaN | NaN | 3 | 3 | 3 | 6 | 1 | 1 | 1.000000 | 2 | 6.0000 | 27.0 | 16.500000 | 33.0000 | 0 | 0 | 0.000000 | 0 | 1 | 1 | 1.0 | 2 | 12.4750 | 12.4750 | 12.475000 | 24.9500 | NaN | NaN | NaN | 0.0 | . 286 3 | 1 | Moor, Mrs. (Beila) | female | 27.0000 | 0 | 1 | 392096 | 12.4750 | E121 | S | 14 | NaN | NaN | 3 | 3 | 3 | 6 | 1 | 1 | 1.000000 | 2 | 6.0000 | 27.0 | 16.500000 | 33.0000 | 0 | 0 | 0.000000 | 0 | 1 | 1 | 1.0 | 2 | 12.4750 | 12.4750 | 12.475000 | 24.9500 | NaN | NaN | NaN | 0.0 | . 287 3 | 1 | Peter, Miss. Anna | female | NaN | 1 | 1 | 2668 | 22.3583 | F E69 | C | D | NaN | NaN | 3 | 3 | 3 | 3 | 1 | 1 | 1.000000 | 1 | NaN | NaN | NaN | 0.0000 | 1 | 1 | 1.000000 | 1 | 1 | 1 | 1.0 | 1 | 22.3583 | 22.3583 | 22.358300 | 22.3583 | NaN | NaN | NaN | 0.0 | . 288 3 | 1 | Pickard, Mr. Berk (Berk Trembisky) | male | 32.0000 | 0 | 0 | SOTON/O.Q. 392078 | 8.0500 | E10 | S | 9 | NaN | NaN | 3 | 3 | 3 | 3 | 1 | 1 | 1.000000 | 1 | 32.0000 | 32.0 | 32.000000 | 32.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 8.0500 | 8.0500 | 8.050000 | 8.0500 | NaN | NaN | NaN | 0.0 | . 289 3 | 1 | Sandstrom, Miss. Beatrice Irene | female | 1.0000 | 1 | 1 | PP 9549 | 16.7000 | G6 | S | 13 | NaN | NaN | 3 | 3 | 3 | 15 | 0 | 1 | 0.600000 | 3 | 1.0000 | 29.0 | 12.000000 | 60.0000 | 0 | 1 | 0.600000 | 3 | 1 | 2 | 1.2 | 6 | 10.4625 | 16.7000 | 14.205000 | 71.0250 | NaN | NaN | NaN | 0.0 | . 290 3 | 1 | Sandstrom, Mrs. Hjalmar (Agnes Charlotta Bengt... | female | 24.0000 | 0 | 2 | PP 9549 | 16.7000 | G6 | S | 13 | NaN | NaN | 3 | 3 | 3 | 15 | 0 | 1 | 0.600000 | 3 | 1.0000 | 29.0 | 12.000000 | 60.0000 | 0 | 1 | 0.600000 | 3 | 1 | 2 | 1.2 | 6 | 10.4625 | 16.7000 | 14.205000 | 71.0250 | NaN | NaN | NaN | 0.0 | . 291 3 | 1 | Sandstrom, Miss. Marguerite Rut | female | 4.0000 | 1 | 1 | PP 9549 | 16.7000 | G6 | S | 13 | NaN | NaN | 3 | 3 | 3 | 15 | 0 | 1 | 0.600000 | 3 | 1.0000 | 29.0 | 12.000000 | 60.0000 | 0 | 1 | 0.600000 | 3 | 1 | 2 | 1.2 | 6 | 10.4625 | 16.7000 | 14.205000 | 71.0250 | NaN | NaN | NaN | 0.0 | . 292 3 | 0 | Strom, Miss. Telma Matilda | female | 2.0000 | 0 | 1 | 347054 | 10.4625 | G6 | S | NaN | NaN | NaN | 3 | 3 | 3 | 15 | 0 | 1 | 0.600000 | 3 | 1.0000 | 29.0 | 12.000000 | 60.0000 | 0 | 1 | 0.600000 | 3 | 1 | 2 | 1.2 | 6 | 10.4625 | 16.7000 | 14.205000 | 71.0250 | NaN | NaN | NaN | 0.0 | . 293 3 | 0 | Strom, Mrs. Wilhelm (Elna Matilda Persson) | female | 29.0000 | 1 | 1 | 347054 | 10.4625 | G6 | S | NaN | NaN | NaN | 3 | 3 | 3 | 15 | 0 | 1 | 0.600000 | 3 | 1.0000 | 29.0 | 12.000000 | 60.0000 | 0 | 1 | 0.600000 | 3 | 1 | 2 | 1.2 | 6 | 10.4625 | 16.7000 | 14.205000 | 71.0250 | NaN | NaN | NaN | 0.0 | . 294 3 | 0 | Tobin, Mr. Roger | male | NaN | 0 | 0 | 383121 | 7.7500 | F38 | Q | NaN | NaN | NaN | 3 | 3 | 3 | 3 | 0 | 0 | 0.000000 | 0 | NaN | NaN | NaN | 0.0000 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0.0 | 0 | 7.7500 | 7.7500 | 7.750000 | 7.7500 | NaN | NaN | NaN | 0.0 | . 295 rows × 42 columns .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter7/",
            "relUrl": "/chapter7/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "6장 탐색",
            "content": "6.1 &#45936;&#51060;&#53552;&#51032; &#53356;&#44592; . X.shape . (1309, 8) . 6.2 &#50836;&#50557; &#53685;&#44228; . X.describe().iloc[:, [0, -1]] . pclass embarked_S . count 1309.000000 | 1309.000000 | . mean -0.012831 | 0.698243 | . std 0.995822 | 0.459196 | . min -1.551881 | 0.000000 | . 25% -0.363317 | 0.000000 | . 50% 0.825248 | 1.000000 | . 75% 0.825248 | 1.000000 | . max 0.825248 | 1.000000 | . 6.3 &#55176;&#49828;&#53664;&#44536;&#47016; . fig, ax = plt.subplots(figsize=(6, 4)) X.fare.plot(kind=&quot;hist&quot;, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd70c5eae10&gt; . import seaborn as sns fig, ax = plt.subplots(figsize=(12, 8)) mask = y_train == 1 ax = sns.distplot(X_train[mask].fare, label=&#39;survived&#39;) ax = sns.distplot(X_train[~mask].fare, label=&#39;died&#39;) ax.set_xlim(-1.5, 1.5) ax.legend() . &lt;matplotlib.legend.Legend at 0x7fd70c5eaa10&gt; . 6.4 &#49328;&#51216;&#46020; . fig, ax = plt.subplots(figsize=(6, 4)) X.plot.scatter(x=&quot;age&quot;, y=&quot;fare&quot;, ax=ax, alpha=0.3) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd6face1d10&gt; . X.age.corr(X.fare) . 0.1771997483998958 . 6.5 &#51312;&#51064;&#53944; &#54540;&#47215; . from yellowbrick.features import ( JointPlotVisualizer, ) fig, ax = plt.subplots(figsize=(6, 6)) jpv = JointPlotVisualizer(feature=&quot;age&quot;, target=&quot;fare&quot;) jpv.fit(X[&quot;age&quot;], X[&quot;fare&quot;]) jpv.poof() . from seaborn import jointplot fig, ax = plt.subplots(figsize=(6, 6)) new_df = X.copy() new_df[&quot;target&quot;] = y p = jointplot(&quot;age&quot;, &quot;fare&quot;, data=new_df, kind=&quot;reg&quot;) . 6.6 &#49933; &#44201;&#51088; . from seaborn import pairplot fig, ax = plt.subplots(figsize=(6, 6)) new_df = X.copy() new_df[&quot;target&quot;] = y vars = [&quot;pclass&quot;, &quot;age&quot;, &quot;fare&quot;] p = pairplot(new_df, vars=vars, hue=&quot;target&quot;, kind=&quot;reg&quot;) . 6.7 &#48149;&#49828; &#54540;&#47215;&#44284; &#48148;&#51060;&#50732;&#47536; &#54540;&#47215; . from seaborn import boxplot fig, ax = plt.subplots(figsize=(8, 6)) new_df = X.copy() new_df[&quot;target&quot;] = y boxplot(x=&quot;target&quot;, y=&quot;age&quot;, data=new_df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd6fbbe6810&gt; . from seaborn import violinplot fig, ax = plt.subplots(figsize=(8, 6)) new_df = X.copy() new_df[&quot;target&quot;] = y violinplot(x=&quot;target&quot;, y=&quot;sex_male&quot;, data=new_df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd6fbb7d910&gt; . 6.8 &#46160; &#49692;&#49436;&#54805; &#44050;&#51032; &#48708;&#44368; . fig, ax = plt.subplots(figsize=(8, 6)) ( X.assign( age_bin=pd.qcut(X.age, q=10, labels=False), class_bin=pd.cut(X.pclass, bins=3, labels=False), ) .groupby([&quot;age_bin&quot;, &quot;class_bin&quot;]) .size() .unstack() .pipe(lambda df: df.div(df.sum(1), axis=0)) .plot.bar(stacked=True, width=1, ax=ax, cmap=&quot;viridis&quot;) .legend(bbox_to_anchor=(1, 1)) ) . &lt;matplotlib.legend.Legend at 0x7fd6faa4c610&gt; . 6.9 &#49345;&#44288;&#44288;&#44228; . from yellowbrick.features import Rank2D fig, ax = plt.subplots(figsize=(6, 6)) pcv = Rank2D(features=X.columns, algorithm=&quot;pearson&quot;) pcv.fit(X, y) pcv.transform(X) pcv.poof() . from seaborn import heatmap fig, ax = plt.subplots(figsize=(8, 8)) ax = heatmap( X.corr(), fmt=&quot;.2f&quot;, annot=True, ax=ax, cmap=&quot;RdBu_r&quot;, vmin=-1, vmax=1, ) . X.corr().iloc[:, :2] . pclass age . pclass 1.000000 | -0.439704 | . age -0.439704 | 1.000000 | . sibsp 0.060832 | -0.292056 | . parch 0.018322 | -0.176447 | . fare -0.558827 | 0.177200 | . sex_male 0.124617 | 0.065004 | . embarked_Q 0.230491 | -0.053904 | . embarked_S 0.096335 | -0.045361 | . import numpy as np def correlated_columns(df, threshold=0.95): return ( df.corr().pipe( lambda df1: pd.DataFrame( np.tril(df1, k=-1), columns=df.columns, index=df.columns ) ) .stack() .rename(&quot;pearson&quot;) .pipe( lambda s: s[ s.abs() &gt; threshold ].reset_index() ) .query(&quot;level_0 not in level_1&quot;) ) correlated_columns(X) . level_0 level_1 pearson . 6.10 &#46972;&#46300;&#48708;&#51592; . from yellowbrick.features import RadViz fig, ax = plt.subplots(figsize=(6, 6)) rv = RadViz( classes=[&quot;died&quot;, &quot;survived&quot;], features=X.columns, ) rv.fit(X, y) _ = rv.transform(X) rv.poof() . from pandas.plotting import radviz fig, ax = plt.subplots(figsize=(6, 6)) new_df = X.copy() new_df[&quot;target&quot;] = y radviz(new_df, &quot;target&quot;, ax=ax, colormap=&quot;PiYG&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd6fa9b63d0&gt; . 6.11 &#54217;&#54665; &#51340;&#54364; . from yellowbrick.features import ( ParallelCoordinates, ) fig, ax = plt.subplots(figsize=(6, 4)) pc = ParallelCoordinates( classes=[&quot;died&quot;, &quot;survived&quot;], features=X.columns, ) pc.fit(X, y) pc.transform(X) ax.set_xticklabels( ax.get_xticklabels(), rotation=45 ) pc.poof() . from pandas.plotting import ( parallel_coordinates, ) fig, ax = plt.subplots(figsize=(6, 4)) new_df = X.copy() new_df[&quot;target&quot;] = y parallel_coordinates( new_df, &quot;target&quot;, ax=ax, colormap=&quot;viridis&quot;, alpha=0.5, ) ax.set_xticklabels( ax.get_xticklabels(), rotation=45 ) . [Text(0,0,&#39;pclass&#39;), Text(0,0,&#39;age&#39;), Text(0,0,&#39;sibsp&#39;), Text(0,0,&#39;parch&#39;), Text(0,0,&#39;fare&#39;), Text(0,0,&#39;sex_male&#39;), Text(0,0,&#39;embarked_Q&#39;), Text(0,0,&#39;embarked_S&#39;)] .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter6/",
            "relUrl": "/chapter6/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "5장 데이터의 정리",
            "content": "&#54596;&#50836;&#54620; &#54056;&#53412;&#51648; . 5.1 &#50676;&#51032; &#51060;&#47492; . import janitor as jn Xbad = pd.DataFrame( { &quot;A&quot;: [1, None, 3], &quot; sales numbers &quot;: [20.0, 30.0, None], } ) jn.clean_names(Xbad) . a _sales_numbers_ . 0 1.0 | 20.0 | . 1 NaN | 30.0 | . 2 3.0 | NaN | . def clean_col(name): return ( name.strip().lower().replace(&quot; &quot;, &quot;_&quot;) ) Xbad.rename(columns=clean_col) . a sales_numbers . 0 1.0 | 20.0 | . 1 NaN | 30.0 | . 2 3.0 | NaN | . 5.2 &#45572;&#46973;&#46108; &#44050;&#51032; &#44368;&#52404; . jn.coalesce( Xbad, columns=[&quot;A&quot;, &quot; sales numbers &quot;], new_column_name=&quot;val&quot;, ) . val . 0 1.0 | . 1 30.0 | . 2 3.0 | . Xbad.fillna(10) . A sales numbers . 0 1.0 | 20.0 | . 1 10.0 | 30.0 | . 2 3.0 | 10.0 | . jn.fill_empty( Xbad, columns=[&quot;A&quot;, &quot; sales numbers &quot;], value=10, ) . A sales numbers . 0 1.0 | 20.0 | . 1 10.0 | 30.0 | . 2 3.0 | 10.0 | . import pandas as pd url = &quot;https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3.xls&quot; df = pd.read_excel(url) orig_df = df df.isna().any().any() . True .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter5/",
            "relUrl": "/chapter5/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "4장 누락된 데이터",
            "content": "&#54596;&#50836;&#54620; &#54056;&#53412;&#51648; . &#45936;&#51060;&#53552;&#51032; &#49688;&#51665; . url = &quot;https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3.xls&quot; df = pd.read_excel(url) orig_df = df . 4.1 &#45572;&#46973;&#46108; &#45936;&#51060;&#53552;&#51032; &#48516;&#49437; . df.isnull().mean() * 100 . pclass 0.000000 survived 0.000000 name 0.000000 sex 0.000000 age 20.091673 sibsp 0.000000 parch 0.000000 ticket 0.000000 fare 0.076394 cabin 77.463713 embarked 0.152788 boat 62.872422 body 90.756303 home.dest 43.086325 dtype: float64 . import missingno as msno ax = msno.matrix(orig_df.sample(500)) plt.show() . fig, ax = plt.subplots(figsize=(6, 4)) (1 - df.isnull().mean()).abs().plot.bar(ax=ax) plt.show() . ax = msno.bar(orig_df.sample(500)) plt.show() . ax = msno.heatmap(df, figsize=(6, 6)) plt.show() . ax = msno.dendrogram(df) plt.show() . 4.2 &#45572;&#46973;&#46108; &#45936;&#51060;&#53552;&#51032; &#49325;&#51228; . df1 = df.dropna() . df1 = df.drop(columns=&quot;cabin&quot;) . df1 = df.dropna(axis=1) . 4.3 &#45936;&#51060;&#53552;&#51032; &#45824;&#52824; . from sklearn.impute import SimpleImputer num_cols = df.select_dtypes( include=&quot;number&quot; ).columns im = SimpleImputer() # 평균 imputed = im.fit_transform(df[num_cols]) . 4.4 &#51648;&#49884;&#51088; &#50676;&#51032; &#52628;&#44032; . def add_indicator(col): def wrapper(df): return df[col].isna().astype(int) return wrapper df1 = df.assign(cabin_missing=add_indicator(&quot;cabin&quot;)) . df1.head(10) . pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest cabin_missing . 0 1 | 1 | Allen, Miss. Elisabeth Walton | female | 29.0000 | 0 | 0 | 24160 | 211.3375 | B5 | S | 2 | NaN | St Louis, MO | 0 | . 1 1 | 1 | Allison, Master. Hudson Trevor | male | 0.9167 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | 11 | NaN | Montreal, PQ / Chesterville, ON | 0 | . 2 1 | 0 | Allison, Miss. Helen Loraine | female | 2.0000 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | NaN | Montreal, PQ / Chesterville, ON | 0 | . 3 1 | 0 | Allison, Mr. Hudson Joshua Creighton | male | 30.0000 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | 135.0 | Montreal, PQ / Chesterville, ON | 0 | . 4 1 | 0 | Allison, Mrs. Hudson J C (Bessie Waldo Daniels) | female | 25.0000 | 1 | 2 | 113781 | 151.5500 | C22 C26 | S | NaN | NaN | Montreal, PQ / Chesterville, ON | 0 | . 5 1 | 1 | Anderson, Mr. Harry | male | 48.0000 | 0 | 0 | 19952 | 26.5500 | E12 | S | 3 | NaN | New York, NY | 0 | . 6 1 | 1 | Andrews, Miss. Kornelia Theodosia | female | 63.0000 | 1 | 0 | 13502 | 77.9583 | D7 | S | 10 | NaN | Hudson, NY | 0 | . 7 1 | 0 | Andrews, Mr. Thomas Jr | male | 39.0000 | 0 | 0 | 112050 | 0.0000 | A36 | S | NaN | NaN | Belfast, NI | 0 | . 8 1 | 1 | Appleton, Mrs. Edward Dale (Charlotte Lamson) | female | 53.0000 | 2 | 0 | 11769 | 51.4792 | C101 | S | D | NaN | Bayside, Queens, NY | 0 | . 9 1 | 0 | Artagaveytia, Mr. Ramon | male | 71.0000 | 0 | 0 | PC 17609 | 49.5042 | NaN | C | NaN | 22.0 | Montevideo, Uruguay | 1 | .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter4/",
            "relUrl": "/chapter4/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "3장 분류 문제 둘러보기(타이타닉 데이터셋)",
            "content": "3.2 &#54596;&#50836;&#54620; &#54056;&#53412;&#51648; . 3.5 &#45936;&#51060;&#53552;&#51032; &#49688;&#51665; . url = &quot;https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3.xls&quot; df = pd.read_excel(url) orig_df = df . df.columns . Index([&#39;pclass&#39;, &#39;survived&#39;, &#39;name&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;ticket&#39;, &#39;fare&#39;, &#39;cabin&#39;, &#39;embarked&#39;, &#39;boat&#39;, &#39;body&#39;, &#39;home.dest&#39;], dtype=&#39;object&#39;) . 3.6 &#45936;&#51060;&#53552;&#51032; &#51221;&#47532; . df.dtypes . pclass int64 survived int64 name object sex object age float64 sibsp int64 parch int64 ticket object fare float64 cabin object embarked object boat object body float64 home.dest object dtype: object . df.shape . (1309, 14) . df.describe().iloc[:, :2] . pclass survived . count 1309.000000 | 1309.000000 | . mean 2.294882 | 0.381971 | . std 0.837836 | 0.486055 | . min 1.000000 | 0.000000 | . 25% 2.000000 | 0.000000 | . 50% 3.000000 | 0.000000 | . 75% 3.000000 | 1.000000 | . max 3.000000 | 1.000000 | . df.isnull().sum() . pclass 0 survived 0 name 0 sex 0 age 263 sibsp 0 parch 0 ticket 0 fare 1 cabin 1014 embarked 2 boat 823 body 1188 home.dest 564 dtype: int64 . mask = df.isnull().any(axis=1) . mask.head() . 0 True 1 True 2 True 3 True 4 True dtype: bool . df[mask].body.head() . 0 NaN 1 NaN 2 NaN 3 135.0 4 NaN Name: body, dtype: float64 . df.sex.value_counts(dropna=False) . male 843 female 466 Name: sex, dtype: int64 . df.embarked.value_counts(dropna=False) . S 914 C 270 Q 123 NaN 2 Name: embarked, dtype: int64 . 3.7 &#53945;&#51669;&#51032; &#49373;&#49457; . name = df.name name.head(3) . 0 Allen, Miss. Elisabeth Walton 1 Allison, Master. Hudson Trevor 2 Allison, Miss. Helen Loraine Name: name, dtype: object . df = df.drop( columns=[&quot;name&quot;, &quot;ticket&quot;, &quot;home.dest&quot;, &quot;boat&quot;, &quot;body&quot;, &quot;cabin&quot;] ) . df = pd.get_dummies(df) . df.columns . Index([&#39;pclass&#39;, &#39;survived&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;fare&#39;, &#39;sex_female&#39;, &#39;sex_male&#39;, &#39;embarked_C&#39;, &#39;embarked_Q&#39;, &#39;embarked_S&#39;], dtype=&#39;object&#39;) . df = df.drop(columns=&quot;sex_male&quot;) . df = pd.get_dummies(df, drop_first=True) . df.columns . Index([&#39;pclass&#39;, &#39;survived&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;fare&#39;, &#39;sex_female&#39;, &#39;embarked_C&#39;, &#39;embarked_Q&#39;, &#39;embarked_S&#39;], dtype=&#39;object&#39;) . y = df.survived X = df.drop(columns=&quot;survived&quot;) . 3.8 &#49368;&#54540; &#45936;&#51060;&#53552; . X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42) . 3.9 &#45936;&#51060;&#53552;&#51032; &#45824;&#52824; . from sklearn.experimental import ( enable_iterative_imputer, ) from sklearn import impute . num_cols = [ &quot;pclass&quot;, &quot;age&quot;, &quot;sibsp&quot;, &quot;parch&quot;, &quot;fare&quot;, &quot;sex_female&quot;, ] . imputer = impute.IterativeImputer() imputed = imputer.fit_transform( X_train[num_cols] ) X_train.loc[:, num_cols] = imputed imputed = imputer.transform(X_test[num_cols]) X_test.loc[:, num_cols] = imputed . meds = X_train.median() X_train = X_train.fillna(meds) X_test = X_test.fillna(meds) . X_train.head() . pclass age sibsp parch fare sex_female embarked_C embarked_Q embarked_S . 1214 3.0 | 26.984481 | 0.0 | 0.0 | 8.6625 | 0.0 | 0 | 0 | 1 | . 677 3.0 | 26.000000 | 0.0 | 0.0 | 7.8958 | 0.0 | 0 | 0 | 1 | . 534 2.0 | 19.000000 | 0.0 | 0.0 | 26.0000 | 1.0 | 0 | 0 | 1 | . 1174 3.0 | 0.437798 | 8.0 | 2.0 | 69.5500 | 1.0 | 0 | 0 | 1 | . 864 3.0 | 28.000000 | 0.0 | 0.0 | 7.7750 | 1.0 | 0 | 0 | 1 | . 3.10 &#45936;&#51060;&#53552;&#51032; &#54364;&#51456;&#54868; . cols = &quot;pclass,age,sibsp,fare&quot;.split(&quot;,&quot;) sca = preprocessing.StandardScaler() . X_train = sca.fit_transform(X_train) X_train = pd.DataFrame(X_train[:, :4], columns=cols) X_test = sca.transform(X_test) X_test = pd.DataFrame(X_test[:, :4], columns=cols) . 3.11 &#47532;&#54057;&#53552;&#47553; . def tweak_titanic(df): df = df.drop( columns=[ &quot;name&quot;, &quot;ticket&quot;, &quot;home.dest&quot;, &quot;boat&quot;, &quot;body&quot;, &quot;cabin&quot;, ] ).pipe(pd.get_dummies, drop_first=True) return df def get_train_test_X_y(df, y_col, size=0.3, std_cols=None): y = df[y_col] X = df.drop(columns=y_col) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=size, random_state=42 ) cols = X.columns num_cols = [ &quot;pclass&quot;, &quot;age&quot;, &quot;sibsp&quot;, &quot;parch&quot;, &quot;fare&quot;, ] fi = impute.IterativeImputer() X_train.loc[:, num_cols] = fi.fit_transform(X_train[num_cols]) X_test.loc[:, num_cols] = fi.transform(X_test[num_cols]) if std_cols: std = preprocessing.StandardScaler() X_train.loc[:, std_cols] = std.fit_transform(X_train[std_cols]) X_test.loc[:, std_cols] = std.transform(X_test[std_cols]) return X_train, X_test, y_train, y_test . ti_df = tweak_titanic(orig_df) std_cols = &quot;pclass,age,sibsp,fare&quot;.split(&quot;,&quot;) X_train, X_test, y_train, y_test = get_train_test_X_y(ti_df, &quot;survived&quot;, std_cols=std_cols) . 3.12 &#48288;&#51060;&#49828;&#46972;&#51064; &#47784;&#45944; . from sklearn.dummy import DummyClassifier bm = DummyClassifier() bm.fit(X_train, y_train) bm.score(X_test, y_test) # 정확도 . 0.5623409669211196 . from sklearn import metrics metrics.precision_score(y_test, bm.predict(X_test)) . 0.44 . 3.13 &#45796;&#50577;&#54620; &#50508;&#44256;&#47532;&#51608; . X = pd.concat([X_train, X_test]) y = pd.concat([y_train, y_test]) . from sklearn import model_selection from sklearn.dummy import DummyClassifier from sklearn.linear_model import ( LogisticRegression, ) from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import ( KNeighborsClassifier, ) from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC from sklearn.ensemble import ( RandomForestClassifier, ) import xgboost . for model in [ DummyClassifier, LogisticRegression, DecisionTreeClassifier, KNeighborsClassifier, GaussianNB, SVC, RandomForestClassifier, xgboost.XGBClassifier, ]: cls = model() kfold = model_selection.KFold(n_splits=10, random_state=42) s = model_selection.cross_val_score(cls, X, y, scoring=&quot;roc_auc&quot;, cv=kfold) print(f&quot;{model.__name__:22} AUC: {s.mean():.3f} STD: {s.std():.2f}&quot;) . DummyClassifier AUC: 0.523 STD: 0.03 LogisticRegression AUC: 0.843 STD: 0.03 DecisionTreeClassifier AUC: 0.762 STD: 0.03 KNeighborsClassifier AUC: 0.830 STD: 0.05 GaussianNB AUC: 0.817 STD: 0.04 SVC AUC: 0.837 STD: 0.05 RandomForestClassifier AUC: 0.845 STD: 0.03 XGBClassifier AUC: 0.863 STD: 0.04 . 3.14 &#49828;&#53468;&#53433; . from mlxtend.classifier import ( StackingClassifier, ) clfs = [ x() for x in [ LogisticRegression, DecisionTreeClassifier, KNeighborsClassifier, GaussianNB, SVC, RandomForestClassifier, ] ] stack = StackingClassifier( classifiers=clfs, meta_classifier=LogisticRegression(), ) kfold = model_selection.KFold(n_splits=10, random_state=42) s = model_selection.cross_val_score(stack, X, y, scoring=&quot;roc_auc&quot;, cv=kfold) print(f&quot;{stack.__class__.__name__} AUC: {s.mean():.3f} STD: {s.std():.2f}&quot;) . StackingClassifier AUC: 0.785 STD: 0.04 . 3.15 &#47784;&#45944; &#47564;&#46308;&#44592; . rf = ensemble.RandomForestClassifier(n_estimators=100, random_state=42) rf.fit(X_train, y_train) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) . 3.16 &#47784;&#45944;&#51032; &#54217;&#44032; . rf.score(X_test, y_test) . 0.7837150127226463 . metrics.precision_score(y_test, rf.predict(X_test)) . 0.7916666666666666 . for col, val in sorted(zip(X_train.columns, rf.feature_importances_), key=lambda x: x[1], reverse=True)[:5]: print(f&quot;{col:10}{val:10.3f}&quot;) . age 0.285 fare 0.262 sex_male 0.241 pclass 0.089 sibsp 0.050 . 3.17 &#47784;&#45944;&#51032; &#52572;&#51201;&#54868; . rf4 = ensemble.RandomForestClassifier() params = { &quot;max_features&quot;: [0.4, &quot;auto&quot;], &quot;n_estimators&quot;: [15, 200], &quot;min_samples_leaf&quot;: [1, 0.1], &quot;random_state&quot;: [42], } cv = model_selection.GridSearchCV(rf4, params, n_jobs=-1).fit(X_train, y_train) print(cv.best_params_) . {&#39;max_features&#39;: 0.4, &#39;min_samples_leaf&#39;: 1, &#39;n_estimators&#39;: 200, &#39;random_state&#39;: 42} . rf5 = ensemble.RandomForestClassifier( **{ &quot;max_features&quot;: &quot;auto&quot;, &quot;min_samples_leaf&quot;: 0.1, &quot;n_estimators&quot;: 200, &quot;random_state&quot;: 42, } ) rf5.fit(X_train, y_train) rf5.score(X_test, y_test) . 0.7073791348600509 . 3.18 &#50724;&#52264; &#54665;&#47148; . from sklearn.metrics import confusion_matrix y_pred = rf5.predict(X_test) confusion_matrix(y_test, y_pred) . array([[217, 7], [108, 61]]) . mapping = {0: &quot;died&quot;, 1: &quot;survived&quot;} fig, ax = plt.subplots(figsize=(6, 6)) cm_viz = ConfusionMatrix( rf5, classes=[&quot;died&quot;, &quot;survived&quot;], label_encoder=mapping, ) cm_viz.score(X_test, y_test) cm_viz.poof() plt.show() . 3.19 ROC &#44257;&#49440; . y_pred = rf5.predict(X_test) roc_auc_score(y_test, y_pred) . 0.6648483727810651 . fig, ax = plt.subplots(figsize=(6, 6)) roc_viz = ROCAUC(rf5) roc_viz.score(X_test, y_test) roc_viz.poof() plt.show() . 3.20 &#54617;&#49845; &#44257;&#49440; . import numpy as np fig, ax = plt.subplots(figsize=(6, 4)) cv = StratifiedKFold(12) sizes = np.linspace(0.3, 1.0, 10) lc_viz = LearningCurve( rf5, cv=cv, train_sizes=sizes, scoring=&quot;f1_weighted&quot;, n_jobs=4, ax=ax, ) lc_viz.fit(X, y) lc_viz.poof() plt.show() . 3.21 &#47784;&#45944;&#51032; &#48176;&#54252; . import pickle pic = pickle.dumps(rf5) rf6 = pickle.loads(pic) y_pred = rf6.predict(X_test) roc_auc_score(y_test, y_pred) . 0.6648483727810651 .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter3/",
            "relUrl": "/chapter3/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "19장 파이프라인",
            "content": "19.1 &#48516;&#47448; &#54028;&#51060;&#54532;&#46972;&#51064; . import pandas as pd from sklearn.experimental import ( enable_iterative_imputer, ) from sklearn import ( ensemble, impute, model_selection, preprocessing, tree, ) from sklearn.base import ( BaseEstimator, TransformerMixin, ) from sklearn.ensemble import ( RandomForestClassifier, ) from sklearn.pipeline import Pipeline def tweak_titanic(df): df = df.drop( columns=[ &quot;name&quot;, &quot;ticket&quot;, &quot;home.dest&quot;, &quot;boat&quot;, &quot;body&quot;, &quot;cabin&quot;, ] ).pipe(pd.get_dummies, drop_first=True) return df class TitanicTransformer( BaseEstimator, TransformerMixin ): def transform(self, X): # assumes X is output # from reading Excel file X = tweak_titanic(X) X = X.drop(columns=&quot;survived&quot;) return X def fit(self, X, y): return self pipe = Pipeline( [ (&quot;titan&quot;, TitanicTransformer()), (&quot;impute&quot;, impute.IterativeImputer()), ( &quot;std&quot;, preprocessing.StandardScaler(), ), (&quot;rf&quot;, RandomForestClassifier()), ] ) . from sklearn.model_selection import ( train_test_split, ) url = &quot;https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3.xls&quot; df = pd.read_excel(url) orig_df = df . from sklearn.model_selection import ( train_test_split, ) X_train2, X_test2, y_train2, y_test2 = train_test_split( orig_df, orig_df.survived, test_size=0.3, random_state=42, ) pipe.fit(X_train2, y_train2) pipe.score(X_test2, y_test2) . 0.7938931297709924 . params = { &quot;rf__max_features&quot;: [0.4, &quot;auto&quot;], &quot;rf__n_estimators&quot;: [15, 200], } grid = model_selection.GridSearchCV( pipe, cv=3, param_grid=params ) grid.fit(orig_df, orig_df.survived) . GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=Pipeline(memory=None, steps=[(&#39;titan&#39;, TitanicTransformer()), (&#39;impute&#39;, IterativeImputer(add_indicator=False, estimator=None, imputation_order=&#39;ascending&#39;, initial_strategy=&#39;mean&#39;, max_iter=10, max_value=None, min_value=None, missing_values=nan, n_nearest_features=None, random_state=None, sample_poster... min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False))], verbose=False), iid=&#39;warn&#39;, n_jobs=None, param_grid={&#39;rf__max_features&#39;: [0.4, &#39;auto&#39;], &#39;rf__n_estimators&#39;: [15, 200]}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . grid.best_params_ . {&#39;rf__max_features&#39;: &#39;auto&#39;, &#39;rf__n_estimators&#39;: 200} . pipe.fit(X_train2, y_train2) pipe.score(X_test2, y_test2) . 0.7608142493638677 . from sklearn import metrics metrics.roc_auc_score( y_test2, pipe.predict(X_test2) ) . 0.7429601648351648 . 19.2 &#54924;&#44480; &#54028;&#51060;&#54532;&#46972;&#51064; . from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression reg_pipe = Pipeline( [ ( &quot;std&quot;, preprocessing.StandardScaler(), ), (&quot;lr&quot;, LinearRegression()), ] ) reg_pipe.fit(bos_X_train, bos_y_train) reg_pipe.score(bos_X_test, bos_y_test) . 0.7112260057484932 . reg_pipe.named_steps[&quot;lr&quot;].intercept_ . 23.01581920903956 . reg_pipe.named_steps[&quot;lr&quot;].coef_ . array([-1.10834602, 0.80843998, 0.34313466, 0.81386426, -1.79804295, 2.913858 , -0.29893918, -2.94251148, 2.09419303, -1.44706731, -2.05232232, 1.02375187, -3.88579002]) . metrics.mean_squared_error( bos_y_test, reg_pipe.predict(bos_X_test) ) . 21.517444231177215 . 19.3 PCA &#54028;&#51060;&#54532;&#46972;&#51064; . from sklearn.decomposition import PCA pca_pipe = Pipeline( [ ( &quot;std&quot;, preprocessing.StandardScaler(), ), (&quot;pca&quot;, PCA()), ] ) X_pca = pca_pipe.fit_transform(X) . pca_pipe.named_steps[&quot;pca&quot;].explained_variance_ratio_ . array([0.23922833, 0.21616853, 0.1923158 , 0.10464906, 0.08154797, 0.0727221 , 0.05130716, 0.04206107]) . pca_pipe.named_steps[&quot;pca&quot;].components_[0] . array([-0.63274156, 0.39602149, 0.00653646, 0.11500362, 0.5815031 , -0.19764926, -0.20422289, -0.10304598]) .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter19/",
            "relUrl": "/chapter19/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "18장 클러스터링",
            "content": "18.1 K-&#54217;&#44512; . from sklearn.cluster import KMeans X_std = preprocessing.StandardScaler().fit_transform(X) km = KMeans(2, random_state=42) km.fit(X_std) . KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300, n_clusters=2, n_init=10, n_jobs=None, precompute_distances=&#39;auto&#39;, random_state=42, tol=0.0001, verbose=0) . X_km = km.predict(X) X_km . array([1, 1, 1, ..., 1, 1, 1], dtype=int32) . inertias = [] sizes = range(2, 12) for k in sizes: k2 = KMeans(random_state=42, n_clusters=k) k2.fit(X) inertias.append(k2.inertia_) fig, ax = plt.subplots(figsize=(8, 8)) pd.Series(inertias, index=sizes).plot(ax=ax) ax.set_xlabel(&quot;K&quot;) ax.set_ylabel(&quot;Inertia&quot;) . Text(0,0.5,&#39;Inertia&#39;) . from sklearn import metrics inertias = [] sils = [] chs = [] dbs = [] sizes = range(2, 12) for k in sizes: k2 = KMeans(random_state=42, n_clusters=k) k2.fit(X_std) inertias.append(k2.inertia_) sils.append( metrics.silhouette_score(X, k2.labels_) ) chs.append( metrics.calinski_harabasz_score( X, k2.labels_ ) ) dbs.append( metrics.davies_bouldin_score( X, k2.labels_ ) ) fig, ax = plt.subplots(figsize=(10, 10)) ( pd.DataFrame( { &quot;inertia&quot;: inertias, &quot;silhouette&quot;: sils, &quot;calinski&quot;: chs, &quot;davis&quot;: dbs, &quot;k&quot;: sizes, } ) .set_index(&quot;k&quot;) .plot(ax=ax, subplots=True, layout=(2, 2)) ) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f96cbb49310&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f96cbb92f50&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f96c059d890&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f96c054f1d0&gt;]], dtype=object) . from yellowbrick.cluster.silhouette import ( SilhouetteVisualizer, ) fig, axes = plt.subplots(2, 2, figsize=(12, 8)) axes = axes.reshape(4) for i, k in enumerate(range(2, 6)): ax = axes[i] sil = SilhouetteVisualizer( KMeans(n_clusters=k, random_state=42), ax=ax, ) sil.fit(X_std) sil.finalize() ax.set_xlim(-0.2, 0.8) plt.tight_layout() . 18.2 &#51025;&#51665; &#53364;&#47084;&#49828;&#53552;&#47553; . from scipy.cluster import hierarchy fig, ax = plt.subplots(figsize=(20, 10)) dend = hierarchy.dendrogram( hierarchy.linkage(X_std, method=&quot;ward&quot;) ) . from scipy.cluster import hierarchy fig, ax = plt.subplots(figsize=(20, 10)) dend = hierarchy.dendrogram( hierarchy.linkage(X_std, method=&quot;ward&quot;), truncate_mode=&quot;lastp&quot;, p=20, show_contracted=True, ) . from sklearn.cluster import ( AgglomerativeClustering, ) ag = AgglomerativeClustering( n_clusters=4, affinity=&quot;euclidean&quot;, linkage=&quot;ward&quot;, ) ag.fit(X) . AgglomerativeClustering(affinity=&#39;euclidean&#39;, compute_full_tree=&#39;auto&#39;, connectivity=None, distance_threshold=None, linkage=&#39;ward&#39;, memory=None, n_clusters=4, pooling_func=&#39;deprecated&#39;) . 18.3 &#53364;&#47084;&#49828;&#53552;&#51032; &#51060;&#54644; . km = KMeans(n_clusters=2) km.fit(X_std) labels = km.predict(X_std) ( X.assign(cluster=labels, survived=y) .groupby(&quot;cluster&quot;) .agg([&quot;mean&quot;, &quot;var&quot;]) .T ) . cluster 0 1 . pclass mean 0.527478 | -1.420910 | . var 0.265532 | 0.138897 | . age mean -0.281925 | 0.922530 | . var 0.651165 | 1.145415 | . sibsp mean -0.009948 | -0.108926 | . var 1.164827 | 0.303463 | . parch mean 0.387949 | 0.377410 | . var 0.830288 | 0.539488 | . fare mean -0.349293 | 0.882876 | . var 0.056372 | 2.223786 | . sex_male mean 0.678647 | 0.553719 | . var 0.218316 | 0.247797 | . embarked_Q mean 0.123679 | 0.016529 | . var 0.108497 | 0.016301 | . embarked_S mean 0.741015 | 0.586777 | . var 0.192115 | 0.243140 | . survived mean 0.300211 | 0.595041 | . var 0.210307 | 0.241633 | . fig, ax = plt.subplots(figsize=(10, 6)) ( X.assign(cluster=labels, survived=y) .groupby(&quot;cluster&quot;) .mean() .T.plot.bar(ax=ax) ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f96bd6cd290&gt; . from sklearn.decomposition import PCA from sklearn.preprocessing import ( StandardScaler, ) import seaborn as sns fig, ax = plt.subplots(figsize=(10, 8)) pca = PCA(random_state=42) X_pca = pca.fit_transform( StandardScaler().fit_transform(X) ) sns.scatterplot( &quot;PC1&quot;, &quot;PC2&quot;, data=X.assign( PC1=X_pca[:, 0], PC2=X_pca[:, 1], cluster=labels, ), hue=&quot;cluster&quot;, alpha=0.5, ax=ax, ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f96bcecf490&gt; . ( X.assign(cluster=labels) .groupby(&quot;cluster&quot;) .age.describe() .T ) . cluster 0 1 . count 946.000000 | 363.000000 | . mean -0.281925 | 0.922530 | . std 0.806948 | 1.070241 | . min -2.221251 | -2.162722 | . 25% -0.628414 | 0.184938 | . 50% -0.175241 | 0.809247 | . 75% 0.106899 | 1.667672 | . max 3.540599 | 4.008830 | . dt = tree.DecisionTreeClassifier() dt.fit(X, labels) for col, val in sorted(zip(X.columns, dt.feature_importances_), key=lambda col_val: col_val[1], reverse=True): print(f&quot;{col:10}{val:10.3f}&quot;) . pclass 0.902 age 0.077 sex_male 0.013 embarked_S 0.003 fare 0.003 parch 0.003 sibsp 0.000 embarked_Q 0.000 . from IPython.display import Image from io import StringIO from sklearn.tree import export_graphviz import pydotplus dot_data = StringIO() export_graphviz( dt, out_file=dot_data, feature_names=X.columns, class_names=[&quot;0&quot;, &quot;1&quot;], max_depth=2, filled=True, ) g = pydotplus.graph_from_dot_data( dot_data.getvalue() ) Image(g.create_png()) .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter18/",
            "relUrl": "/chapter18/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "17장 차원성 감소",
            "content": "17.1 PCA . from sklearn.decomposition import PCA from sklearn.preprocessing import ( StandardScaler, ) pca = PCA(random_state=42) X_pca = pca.fit_transform(StandardScaler().fit_transform(X)) pca.explained_variance_ratio_ . array([0.23922833, 0.21616853, 0.1923158 , 0.10464906, 0.08154797, 0.0727221 , 0.05130716, 0.04206107]) . pca.components_[0] . array([-0.63274156, 0.39602149, 0.00653646, 0.11500362, 0.5815031 , -0.19764926, -0.20422289, -0.10304598]) . fig, ax = plt.subplots(figsize=(8, 6)) ax.plot(pca.explained_variance_ratio_) ax.set( xlabel=&quot;Component&quot;, ylabel=&quot;Percent of Explained variance&quot;, title=&quot;Scree Plot&quot;, ylim=(0, 1), ) . [(0, 1), Text(0,0.5,&#39;Percent of Explained variance&#39;), Text(0.5,0,&#39;Component&#39;), Text(0.5,1,&#39;Scree Plot&#39;)] . fig, ax = plt.subplots(figsize=(8, 6)) ax.plot( np.cumsum(pca.explained_variance_ratio_) ) ax.set( xlabel=&quot;Component&quot;, ylabel=&quot;Percent of Explained variance&quot;, title=&quot;Cumulative Variance&quot;, ylim=(0, 1), ) . [(0, 1), Text(0,0.5,&#39;Percent of Explained variance&#39;), Text(0.5,0,&#39;Component&#39;), Text(0.5,1,&#39;Cumulative Variance&#39;)] . fig, ax = plt.subplots(figsize=(8, 8)) plt.imshow( pca.components_.T, cmap=&quot;Spectral&quot;, vmin=-1, vmax=1, ) plt.yticks(range(len(X.columns)), X.columns) plt.xticks(range(8), range(1, 9)) plt.xlabel(&quot;Principal Component&quot;) plt.ylabel(&quot;Contribution&quot;) plt.title(&quot;Contribution of Features to Components&quot;) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7ff9f9d3ac90&gt; . fig, ax = plt.subplots(figsize=(10, 8)) pd.DataFrame( pca.components_, columns=X.columns ).plot(kind=&quot;bar&quot;, ax=ax).legend( bbox_to_anchor=(1, 1) ) . &lt;matplotlib.legend.Legend at 0x7ff9efad6e90&gt; . comps = pd.DataFrame(pca.components_, columns=X.columns) min_val = 0.5 num_components = 2 pca_cols = set() for i in range(num_components): parts = comps.iloc[i][comps.iloc[i].abs() &gt; min_val] pca_cols.update(set(parts.index)) pca_cols . {&#39;fare&#39;, &#39;parch&#39;, &#39;pclass&#39;, &#39;sibsp&#39;} . from yellowbrick.features.pca import ( PCADecomposition, ) fig, ax = plt.subplots(figsize=(10, 8)) colors = [&quot;rg&quot;[j] for j in y] pca_viz = PCADecomposition(color=colors) pca_viz.fit_transform(X, y) pca_viz.poof() . import seaborn as sns fig, ax = plt.subplots(figsize=(10, 8)) pca_df = pd.DataFrame( X_pca, columns=[ f&quot;PC{i+1}&quot; for i in range(X_pca.shape[1]) ] ) pca_df[&quot;status&quot;] = [ (&quot;deceased&quot;, &quot;survived&quot;)[i] for i in y ] evr = pca.explained_variance_ratio_ ax.set_aspect(evr[1] / evr[0]) sns.scatterplot( x=&quot;PC1&quot;, y=&quot;PC2&quot;, hue=&quot;status&quot;, data=pca_df, alpha=0.5, ax=ax, ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff9ed051810&gt; . fig, ax = plt.subplots(figsize=(10, 8)) pca_df = pd.DataFrame( X_pca, columns=[ f&quot;PC{i+1}&quot; for i in range(X_pca.shape[1]) ], ) pca_df[&quot;status&quot;] = [ (&quot;deceased&quot;, &quot;survived&quot;)[i] for i in y ] evr = pca.explained_variance_ratio_ x_idx = 0 # x_pc y_idx = 1 # y_pc ax.set_aspect(evr[y_idx] / evr[x_idx]) x_col = pca_df.columns[x_idx] y_col = pca_df.columns[y_idx] sns.scatterplot( x=x_col, y=y_col, hue=&quot;status&quot;, data=pca_df, alpha=0.5, ax=ax, ) scale = 8 comps = pd.DataFrame( pca.components_, columns=X.columns ) for idx, s in comps.T.iterrows(): plt.arrow( 0, 0, s[x_idx] * scale, s[y_idx] * scale, color=&quot;k&quot;, ) plt.text( s[x_idx] * scale, s[y_idx] * scale, idx, weight=&quot;bold&quot;, ) . fig, ax = plt.subplots(figsize=(10, 8)) pca_df = pd.DataFrame( X_pca, columns=[ f&quot;PC{i+1}&quot; for i in range(X_pca.shape[1]) ], ) pca_df[&quot;status&quot;] = [ (&quot;deceased&quot;, &quot;survived&quot;)[i] for i in y ] evr = pca.explained_variance_ratio_ ax.set_aspect(evr[3] / evr[0]) sns.scatterplot( x=&quot;PC1&quot;, y=&quot;PC4&quot;, hue=&quot;status&quot;, data=pca_df, alpha=0.5, ax=ax, ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff9ecf19710&gt; . from bokeh.io import output_notebook from bokeh import models, palettes, transform from bokeh.plotting import figure, show def bokeh_scatter(x, y, data, hue=None, label_cols=None, size=None, legend=None, alpha=0.5,): output_notebook() circle_kwargs = {} if legend: circle_kwargs[&quot;legend&quot;] = legend if size: circle_kwargs[&quot;size&quot;] = size if hue: color_seq = data[hue] mapper = models.LinearColorMapper( palette=palettes.viridis(256), low=min(color_seq), high=max(color_seq), ) circle_kwargs[ &quot;fill_color&quot; ] = transform.transform(hue, mapper) ds = models.ColumnDataSource(data) if label_cols is None: label_cols = data.columns tool_tips = sorted( [ (x, &quot;@{}&quot;.format(x)) for x in label_cols ], key=lambda tup: tup[0], ) hover = models.HoverTool( tooltips=tool_tips ) fig = figure( tools=[ hover, &quot;pan&quot;, &quot;zoom_in&quot;, &quot;zoom_out&quot;, &quot;reset&quot;, ], toolbar_location=&quot;below&quot;, ) fig.circle( x, y, source=ds, alpha=alpha, **circle_kwargs ) show(fig) return fig res = bokeh_scatter( &quot;PC1&quot;, &quot;PC2&quot;, data=pca_df.assign( surv=y.reset_index(drop=True) ), hue=&quot;surv&quot;, size=10, legend=&quot;surv&quot;, ) plt.show() . BokehDeprecationWarning: &#39;legend&#39; keyword is deprecated, use explicit &#39;legend_label&#39;, &#39;legend_field&#39;, or &#39;legend_group&#39; keywords instead . from yellowbrick.features.pca import ( PCADecomposition, ) colors = [&quot;rg&quot;[j] for j in y] pca3_viz = PCADecomposition( proj_dim=3, color=colors ) pca3_viz.fit_transform(X, y) pca3_viz.finalize() fig = plt.gcf() plt.tight_layout() . from mpl_toolkits.mplot3d import Axes3D fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=&quot;3d&quot;) ax.scatter( xs=X_pca[:, 0], ys=X_pca[:, 1], zs=X_pca[:, 2], c=y, cmap=&quot;viridis&quot;, ) ax.set_xlabel(&quot;PC 1&quot;) ax.set_ylabel(&quot;PC 2&quot;) ax.set_zlabel(&quot;PC 3&quot;) . Text(0.5,0,&#39;PC 3&#39;) . 17.2 UMAP . import umap u = umap.UMAP(random_state=42) X_umap = u.fit_transform( StandardScaler().fit_transform(X) ) . X_umap.shape . (1309, 2) . fig, ax = plt.subplots(figsize=(10, 10)) pd.DataFrame(X_umap).plot( kind=&quot;scatter&quot;, x=0, y=1, ax=ax, c=y, alpha=0.2, cmap=&quot;Spectral&quot;, ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff9af559550&gt; . X_std = StandardScaler().fit_transform(X) fig, axes = plt.subplots(2, 2, figsize=(10, 10)) axes = axes.reshape(4) for i, n in enumerate([2, 5, 10, 50]): ax = axes[i] u = umap.UMAP( random_state=42, n_neighbors=n ) X_umap = u.fit_transform(X_std) pd.DataFrame(X_umap).plot( kind=&quot;scatter&quot;, x=0, y=1, ax=ax, c=y, cmap=&quot;Spectral&quot;, alpha=0.5, ) ax.set_title(f&quot;nn={n}&quot;) plt.tight_layout() . fig, axes = plt.subplots(2, 2, figsize=(10, 10)) axes = axes.reshape(4) for i, n in enumerate([0, 0.33, 0.66, 0.99]): ax = axes[i] u = umap.UMAP(random_state=42, min_dist=n) X_umap = u.fit_transform(X_std) pd.DataFrame(X_umap).plot( kind=&quot;scatter&quot;, x=0, y=1, ax=ax, c=y, cmap=&quot;Spectral&quot;, alpha=0.5, ) ax.set_title(f&quot;min_dist={n}&quot;) plt.tight_layout() . 17.3 t-SNE . from sklearn.manifold import TSNE X_std = StandardScaler().fit_transform(X) ts = TSNE() X_tsne = ts.fit_transform(X_std) . fig, ax = plt.subplots(figsize=(10, 10)) colors = [&quot;rg&quot;[j] for j in y] scat = ax.scatter( X_tsne[:, 0], X_tsne[:, 1], c=colors, alpha=0.5, ) ax.set_xlabel(&quot;Embedding 1&quot;) ax.set_ylabel(&quot;Embedding 2&quot;) . Text(0,0.5,&#39;Embedding 2&#39;) . fig, axes = plt.subplots(2, 2, figsize=(10, 10)) axes = axes.reshape(4) for i, n in enumerate((2, 30, 50, 100)): ax = axes[i] t = TSNE(random_state=42, perplexity=n) X_tsne = t.fit_transform(X) pd.DataFrame(X_tsne).plot( kind=&quot;scatter&quot;, x=0, y=1, ax=ax, c=y, cmap=&quot;Spectral&quot;, alpha=0.5, ) ax.set_title(f&quot;perplexity={n}&quot;) plt.tight_layout() .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter17/",
            "relUrl": "/chapter17/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "16장 회귀 모델의 해석",
            "content": "import xgboost as xgb xgr = xgb.XGBRegressor( random_state=42, base_score=0.5 ) xgr.fit(bos_X_train, bos_y_train) . [14:43:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. . XGBRegressor(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, importance_type=&#39;gain&#39;, learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=100, n_jobs=1, nthread=None, objective=&#39;reg:linear&#39;, random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1) . 16.1 &#49520;&#51060;&#54540;&#47532; . sample_idx = 5 xgr.predict(bos_X.iloc[[sample_idx]]) . array([27.269186], dtype=float32) . import shap shap.initjs() exp = shap.TreeExplainer(xgr) vals = exp.shap_values(bos_X) shap.force_plot( exp.expected_value, vals[sample_idx], bos_X.iloc[sample_idx], ) . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. shap.initjs() shap.force_plot( exp.expected_value, vals, bos_X ) . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. import matplotlib.pyplot as plt shap.dependence_plot(&quot;LSTAT&quot;, vals, bos_X) . shap.dependence_plot( &quot;DIS&quot;, vals, bos_X, interaction_index=&quot;RM&quot; ) . shap.summary_plot(vals, bos_X) .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter16/",
            "relUrl": "/chapter16/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "15장 평가 지표와 회귀의 평가",
            "content": "from sklearn.ensemble import ( RandomForestRegressor, ) rfr = RandomForestRegressor( random_state=42, n_estimators=100 ) rfr.fit(bos_X_train, bos_y_train) . RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) . 15.1 &#54217;&#44032; &#51648;&#54364; . from sklearn import metrics rfr.score(bos_X_test, bos_y_test) . 0.8721182042634867 . bos_y_test_pred = rfr.predict(bos_X_test) metrics.r2_score(bos_y_test, bos_y_test_pred) . 0.8721182042634867 . metrics.mean_absolute_error( bos_y_test, bos_y_test_pred ) . 2.0839802631578945 . 15.2 &#51092;&#52264; &#46020;&#54364; . import matplotlib.pyplot as plt from yellowbrick.regressor import ResidualsPlot fig, ax = plt.subplots(figsize=(10, 8)) rpv = ResidualsPlot(rfr) rpv.fit(bos_X_train, bos_y_train) rpv.score(bos_X_test, bos_y_test) rpv.poof() . 15.3 &#51060;&#48516;&#49328;&#49457; . import statsmodels.stats.api as sms resids = bos_y_test - rfr.predict(bos_X_test) hb = sms.het_breuschpagan(resids, bos_X_test) labels = [ &quot;Lagrange multiplier statistic&quot;, &quot;p-value&quot;, &quot;f-value&quot;, &quot;f p-value&quot;, ] for name, num in zip(labels, hb): print(f&quot;{name}: {num:.2}&quot;) . Lagrange multiplier statistic: 3.6e+01 p-value: 0.00036 f-value: 3.3 f p-value: 0.00022 . 15.4 &#51221;&#44508; &#51092;&#52264; . fig, ax = plt.subplots(figsize=(8, 6)) resids = bos_y_test - rfr.predict(bos_X_test) pd.Series(resids, name=&quot;residuals&quot;).plot.hist( bins=20, ax=ax, title=&quot;Residual Histogram&quot; ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f364ad8b290&gt; . from scipy import stats fig, ax = plt.subplots(figsize=(8, 6)) _ = stats.probplot(resids, plot=ax) . stats.kstest(resids, cdf=&quot;norm&quot;) . KstestResult(statistic=0.1962230021010155, pvalue=1.3283596864962378e-05) . 15.5 &#50696;&#52769; &#50724;&#52264; &#46020;&#54364; . from yellowbrick.regressor import ( PredictionError, ) fig, ax = plt.subplots(figsize=(10, 8)) pev = PredictionError(rfr) pev.fit(bos_X_train, bos_y_train) pev.score(bos_X_test, bos_y_test) pev.poof() .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter15/",
            "relUrl": "/chapter15/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "14장 회귀",
            "content": "&#45936;&#51060;&#53552; &#51456;&#48708; . import pandas as pd from sklearn.datasets import load_boston from sklearn import ( model_selection, preprocessing, ) b = load_boston() bos_X = pd.DataFrame( b.data, columns=b.feature_names ) bos_y = b.target bos_X_train, bos_X_test, bos_y_train, bos_y_test = model_selection.train_test_split( bos_X, bos_y, test_size=0.3, random_state=42, ) bos_sX = preprocessing.StandardScaler().fit_transform(bos_X) bos_sX_train, bos_sX_test, bos_sy_train, bos_sy_test = model_selection.train_test_split( bos_sX, bos_y, test_size=0.3, random_state=42, ) . 14.1 &#48288;&#51060;&#49828;&#46972;&#51064; &#47784;&#45944; . from sklearn.dummy import DummyRegressor dr = DummyRegressor() dr.fit(bos_X_train, bos_y_train) dr.score(bos_X_test, bos_y_test) . -0.03469753992352409 . 14.2 &#49440;&#54805; &#54924;&#44480; . from sklearn.linear_model import ( LinearRegression, ) lr = LinearRegression() lr.fit(bos_X_train, bos_y_train) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . lr.score(bos_X_test, bos_y_test) . 0.7112260057484974 . lr.coef_ . array([-1.33470103e-01, 3.58089136e-02, 4.95226452e-02, 3.11983512e+00, -1.54170609e+01, 4.05719923e+00, -1.08208352e-02, -1.38599824e+00, 2.42727340e-01, -8.70223437e-03, -9.10685208e-01, 1.17941159e-02, -5.47113313e-01]) . lr2 = LinearRegression() lr2.fit(bos_sX_train, bos_sy_train) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . lr2.score(bos_sX_test, bos_sy_test) . 0.7112260057484923 . lr2.intercept_ . 22.50945471291039 . lr2.coef_ . array([-1.14691411, 0.83432605, 0.33940694, 0.79163612, -1.784727 , 2.84783949, -0.30429306, -2.91562521, 2.11140045, -1.46519951, -1.9696347 , 1.07567771, -3.90310727]) . import matplotlib.pyplot as plt from yellowbrick.features import ( FeatureImportances, ) fig, ax = plt.subplots(figsize=(10, 8)) fi_viz = FeatureImportances( lr2, labels=bos_X.columns ) fi_viz.fit(bos_sX, bos_y) fi_viz.poof() . 14.3 SVM . from sklearn.svm import SVR svr = SVR() svr.fit(bos_sX_train, bos_sy_train) . SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;auto_deprecated&#39;, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False) . svr.score(bos_sX_test, bos_sy_test) . 0.6553772022206867 . 14.4 K-&#52572;&#44540;&#51217; &#51060;&#50883; . from sklearn.neighbors import ( KNeighborsRegressor, ) knr = KNeighborsRegressor() knr.fit(bos_sX_train, bos_sy_train) . KNeighborsRegressor(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . knr.score(bos_sX_test, bos_sy_test) . 0.7476242635592417 . 14.5 &#46356;&#49884;&#51204; &#53944;&#47532; . from sklearn.tree import DecisionTreeRegressor dtr = DecisionTreeRegressor(random_state=42) dtr.fit(bos_X_train, bos_y_train) . DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=42, splitter=&#39;best&#39;) . dtr.score(bos_X_test, bos_y_test) . 0.8559207694719114 . import pydotplus from IPython.display import Image from io import StringIO from sklearn.tree import export_graphviz dot_data = StringIO() export_graphviz( dtr, out_file=dot_data, feature_names=bos_X.columns, filled=True, ) g = pydotplus.graph_from_dot_data( dot_data.getvalue() ) Image(g.create_png()) . from IPython.display import Image dot_data = StringIO() export_graphviz( dtr, max_depth=2, out_file=dot_data, feature_names=bos_X.columns, filled=True, ) g = pydotplus.graph_from_dot_data( dot_data.getvalue() ) Image(g.create_png()) . from dtreeviz.trees import dtreeviz dtr3 = DecisionTreeRegressor(max_depth=2) dtr3.fit(bos_X_train, bos_y_train) viz = dtreeviz( dtr3, bos_X, bos_y, target_name=&quot;price&quot;, feature_names=bos_X.columns, scale=(2.5) ) viz . G node1 node4 leaf2 node1-&gt;leaf2 leaf3 node1-&gt;leaf3 leaf5 node4-&gt;leaf5 leaf6 node4-&gt;leaf6 node0 node0-&gt;node1 &lt; node0-&gt;node4 &#8805; for col, val in sorted(zip(bos_X.columns, dtr.feature_importances_), key=lambda x: x[1], reverse=True)[:5]: print(f&quot;{col:10}{val:10.3f}&quot;) . RM 0.576 LSTAT 0.192 DIS 0.111 CRIM 0.035 NOX 0.029 . 14.6 &#47004;&#45924; &#54252;&#47112;&#49828;&#53944; . from sklearn.ensemble import ( RandomForestRegressor, ) rfr = RandomForestRegressor( random_state=42, n_estimators=100 ) rfr.fit(bos_X_train, bos_y_train) . RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) . rfr.score(bos_X_test, bos_y_test) . 0.8721182042634867 . for col, val in sorted(zip(bos_X.columns, rfr.feature_importances_), key=lambda x: x[1], reverse=True)[:5]: print(f&quot;{col:10}{val:10.3f}&quot;) . RM 0.439 LSTAT 0.380 DIS 0.067 CRIM 0.032 PTRATIO 0.020 . 14.7 XGBoost &#54924;&#44480; . import xgboost as xgb xgr = xgb.XGBRegressor(random_state=42) xgr.fit(bos_X_train, bos_y_train) . [14:22:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. . XGBRegressor(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, importance_type=&#39;gain&#39;, learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=100, n_jobs=1, nthread=None, objective=&#39;reg:linear&#39;, random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1) . xgr.score(bos_X_test, bos_y_test) . 0.871679473122472 . xgr.predict(bos_X.iloc[[0]]) . array([27.013563], dtype=float32) . for col, val in sorted(zip(bos_X.columns, xgr.feature_importances_), key=lambda x: x[1], reverse=True)[:5]: print(f&quot;{col:10}{val:10.3f}&quot;) . LSTAT 0.485 RM 0.263 PTRATIO 0.060 NOX 0.051 DIS 0.032 . fig, ax = plt.subplots(figsize=(10, 8)) xgb.plot_importance(xgr, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5a0afe6310&gt; . fig, ax = plt.subplots(figsize=(10, 8)) fi_viz = FeatureImportances(xgr) fi_viz.fit(bos_X_train, bos_y_train) fi_viz.poof() . [14:23:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. . booster = xgr.get_booster() print(booster.get_dump()[0]) . 0:[LSTAT&lt;7.86499977] yes=1,no=2,missing=1 1:[RM&lt;7.43700027] yes=3,no=4,missing=3 3:[RM&lt;6.65649986] yes=7,no=8,missing=7 7:leaf=2.42512202 8:leaf=3.10899997 4:[CRIM&lt;2.74223518] yes=9,no=10,missing=9 9:leaf=4.31652212 10:leaf=1.07000005 2:[LSTAT&lt;16.0849991] yes=5,no=6,missing=5 5:[B&lt;47.7250023] yes=11,no=12,missing=11 11:leaf=0.86499995 12:leaf=2.11871624 6:[DIS&lt;1.94000006] yes=13,no=14,missing=13 13:leaf=1.06833339 14:leaf=1.57500005 . fig, ax = plt.subplots(figsize=(30, 20)) xgb.plot_tree(xgr, ax=ax, num_trees=0) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5a0b026b50&gt; . 14.8 LightGBM &#54924;&#44480; &#48516;&#49437; . import lightgbm as lgb lgr = lgb.LGBMRegressor(random_state=42) lgr.fit(bos_X_train, bos_y_train) . LGBMRegressor(boosting_type=&#39;gbdt&#39;, class_weight=None, colsample_bytree=1.0, importance_type=&#39;split&#39;, learning_rate=0.1, max_depth=-1, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31, objective=None, random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0, subsample_for_bin=200000, subsample_freq=0) . lgr.score(bos_X_test, bos_y_test) . 0.847729219534575 . lgr.predict(bos_X.iloc[[0]]) . array([30.31689569]) . for col, val in sorted(zip(bos_X.columns, lgr.feature_importances_), key=lambda x: x[1], reverse=True)[:5]: print(f&quot;{col:10}{val:10.3f}&quot;) . LSTAT 226.000 RM 199.000 DIS 172.000 AGE 130.000 B 121.000 . fig, ax = plt.subplots(figsize=(10, 8)) lgb.plot_importance(lgr, ax=ax) fig.tight_layout() .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter14/",
            "relUrl": "/chapter14/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "13장 모델 설명",
            "content": "from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier( random_state=42, max_depth=3 ) dt.fit(X_train, y_train) . DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=42, splitter=&#39;best&#39;) . 13.3 LIME . from lime import lime_tabular explainer = lime_tabular.LimeTabularExplainer( X_train.values, feature_names=X.columns, class_names=[&quot;died&quot;, &quot;survived&quot;], ) exp = explainer.explain_instance( X_train.iloc[-1].values, dt.predict_proba ) . fig = exp.as_pyplot_figure() fig.tight_layout() . data = X_train.iloc[-2].values.copy() dt.predict_proba( [data] ) . array([[0.48062016, 0.51937984]]) . data[5] = 1 dt.predict_proba([data]) . array([[0.87954545, 0.12045455]]) . 13.4 &#53944;&#47532; &#44592;&#48152; &#47784;&#45944;&#51032; &#54644;&#49437; . rf5 = ensemble.RandomForestClassifier( **{ &quot;max_features&quot;: &quot;auto&quot;, &quot;min_samples_leaf&quot;: 0.1, &quot;n_estimators&quot;: 200, &quot;random_state&quot;: 42, } ) rf5.fit(X_train, y_train) . RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=0.1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) . from treeinterpreter import ( treeinterpreter as ti, ) instances = X.iloc[:2] prediction, bias, contribs = ti.predict( rf5, instances ) i = 0 print(&quot;Instance&quot;, i) print(&quot;Prediction&quot;, prediction[i]) print(&quot;Bias (trainset mean)&quot;, bias[i]) print(&quot;Feature contributions:&quot;) for c, feature in zip( contribs[i], instances.columns ): print(&quot; {} {}&quot;.format(feature, c)) . Instance 0 Prediction [0.82046191 0.17953809] Bias (trainset mean) [0.63887555 0.36112445] Feature contributions: pclass [ 0.02865085 -0.02865085] age [ 0.01115629 -0.01115629] sibsp [ 0.00528926 -0.00528926] parch [ 0.00656872 -0.00656872] fare [ 0.04126856 -0.04126856] sex_male [ 0.07660626 -0.07660626] embarked_Q [0. 0.] embarked_S [ 0.01204643 -0.01204643] . 13.5 &#48512;&#48516; &#51032;&#51316;&#49457; &#46020;&#54364; . rf5 = ensemble.RandomForestClassifier( **{ &quot;max_features&quot;: &quot;auto&quot;, &quot;min_samples_leaf&quot;: 0.1, &quot;n_estimators&quot;: 200, &quot;random_state&quot;: 42, } ) rf5.fit(X_train, y_train) . RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=0.1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) . from pdpbox import pdp feat_name = &quot;age&quot; p = pdp.pdp_isolate( rf5, X, X.columns, feat_name ) fig, _ = pdp.pdp_plot( p, feat_name, plot_lines=True, figsize=(30, 15) ) . features = [&quot;fare&quot;, &quot;sex_male&quot;] p = pdp.pdp_interact( rf5, X, X.columns, features ) fig, _ = pdp.pdp_interact_plot(p, features) . 13.6 &#45824;&#47532; &#47784;&#45944; . from sklearn import svm sv = svm.SVC() sv.fit(X_train, y_train) sur_dt = tree.DecisionTreeClassifier() sur_dt.fit(X_test, sv.predict(X_test)) for col, val in sorted(zip(X_test.columns,sur_dt.feature_importances_), key=lambda x: x[1], reverse=True)[:7]: print(f&quot;{col:10}{val:10.3f}&quot;) . sex_male 0.723 pclass 0.070 age 0.062 sibsp 0.061 embarked_S 0.056 fare 0.022 parch 0.005 . 13.7 &#49520;&#51060;&#54540;&#47532; . rf5.predict_proba(X_test.iloc[[20]]) . array([[0.60129761, 0.39870239]]) . import shap shap.initjs() s = shap.TreeExplainer(rf5) shap_vals = s.shap_values(X_test) target_idx = 1 shap.force_plot( s.expected_value[target_idx], shap_vals[target_idx][20, :], feature_names=X_test.columns, ) . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. shap.initjs() shap.force_plot( s.expected_value[1], shap_vals[1], feature_names=X_test.columns, ) . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. fig, ax = plt.subplots(figsize=(10, 8)) shap.summary_plot(shap_vals[0], X_test) .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter13/",
            "relUrl": "/chapter13/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "12장 메트릭 및 분류",
            "content": "12.1 &#50724;&#52264; &#54665;&#47148; . from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier( random_state=42, max_depth=3 ) dt.fit(X_train, y_train) . DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=42, splitter=&#39;best&#39;) . y_predict = dt.predict(X_test) tp = ((y_test == 1) &amp; (y_test == y_predict)).sum() # 123 tn = ((y_test == 0) &amp; (y_test == y_predict)).sum() # 199 fp = ((y_test == 0) &amp; (y_test != y_predict)).sum() # 25 fn = ((y_test == 1) &amp; (y_test != y_predict)).sum() # 46 . from sklearn.metrics import confusion_matrix y_predict = dt.predict(X_test) pd.DataFrame( confusion_matrix(y_test, y_predict), columns=[ &quot;Predict died&quot;, &quot;Predict Survive&quot;, ], index=[&quot;True Death&quot;, &quot;True Survive&quot;], ) . Predict died Predict Survive . True Death 199 | 25 | . True Survive 48 | 121 | . import matplotlib.pyplot as plt from yellowbrick.classifier import ( ConfusionMatrix, ) mapping = {0: &quot;died&quot;, 1: &quot;survived&quot;} fig, ax = plt.subplots(figsize=(6, 6)) cm_viz = ConfusionMatrix( dt, classes=[&quot;died&quot;, &quot;survived&quot;], label_encoder=mapping, ) cm_viz.score(X_test, y_test) cm_viz.poof() . 12.3 &#51221;&#54869;&#46020; . (tp + tn) / (tp + tn + fp + fn) . 0.8142493638676844 . from sklearn.metrics import accuracy_score y_predict = dt.predict(X_test) accuracy_score(y_test, y_predict) . 0.8142493638676844 . 12.4 &#51116;&#54788;&#50984; . tp / (tp + fn) . 0.7159763313609467 . from sklearn.metrics import recall_score y_predict = dt.predict(X_test) recall_score(y_test, y_predict) . 0.7159763313609467 . 12.5 &#51221;&#48128;&#46020; . tp / (tp + fp) . 0.8287671232876712 . from sklearn.metrics import precision_score y_predict = dt.predict(X_test) precision_score(y_test, y_predict) . 0.8287671232876712 . 12.6 F1 . pre = tp / (tp + fp) rec = tp / (tp + fn) (2 * pre * rec) / (pre + rec) . 0.7682539682539683 . from sklearn.metrics import f1_score y_predict = dt.predict(X_test) f1_score(y_test, y_predict) . 0.7682539682539683 . 12.7 &#48516;&#47448; &#48372;&#44256;&#49436; . import matplotlib.pyplot as plt from yellowbrick.classifier import ( ClassificationReport, ) fig, ax = plt.subplots(figsize=(6, 3)) cm_viz = ClassificationReport( dt, classes=[&quot;died&quot;, &quot;survived&quot;], label_encoder=mapping, ) cm_viz.score(X_test, y_test) cm_viz.poof() . 12.8 ROC . from sklearn.metrics import roc_auc_score y_predict = dt.predict(X_test) roc_auc_score(y_test, y_predict) . 0.8021845942519018 . from yellowbrick.classifier import ROCAUC fig, ax = plt.subplots(figsize=(6, 6)) roc_viz = ROCAUC(dt) roc_viz.score(X_test, y_test) roc_viz.poof() . 12.9 &#51221;&#48128;&#46020;-&#51116;&#54788;&#50984; &#44257;&#49440; . from sklearn.metrics import ( average_precision_score, ) y_predict = dt.predict(X_test) average_precision_score(y_test, y_predict) . 0.7155150490642249 . from yellowbrick.classifier import ( PrecisionRecallCurve, ) fig, ax = plt.subplots(figsize=(6, 4)) viz = PrecisionRecallCurve( DecisionTreeClassifier(max_depth=3) ) viz.fit(X_train, y_train) print(viz.score(X_test, y_test)) viz.poof() . 0.8177126373723864 . 12.10 &#45572;&#51201; &#51060;&#46301; &#46020;&#54364; . import scikitplot fig, ax = plt.subplots(figsize=(6, 6)) y_probas = dt.predict_proba(X_test) scikitplot.metrics.plot_cumulative_gain( y_test, y_probas, ax=ax ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f95c41d89d0&gt; . 12.11 &#47532;&#54532;&#53944; &#44257;&#49440; . fig, ax = plt.subplots(figsize=(6, 6)) y_probas = dt.predict_proba(X_test) scikitplot.metrics.plot_lift_curve( y_test, y_probas, ax=ax ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f95c41d4750&gt; . 12.12 &#48276;&#51452;&#51032; &#44512;&#54805; . from yellowbrick.classifier import ClassBalance fig, ax = plt.subplots(figsize=(6, 6)) cb_viz = ClassBalance( labels=[&quot;Died&quot;, &quot;Survived&quot;] ) cb_viz.fit(y_test) cb_viz.poof() . 12.13 &#48276;&#51452; &#50696;&#52769; &#50724;&#47448; . from yellowbrick.classifier import ( ClassPredictionError, ) fig, ax = plt.subplots(figsize=(6, 6)) cpe_viz = ClassPredictionError( dt, classes=[&quot;died&quot;, &quot;survived&quot;] ) cpe_viz.score(X_test, y_test) cpe_viz.poof() . 12.14 &#52264;&#48324; &#51076;&#44228;&#52824; . from yellowbrick.classifier import ( DiscriminationThreshold, ) fig, ax = plt.subplots(figsize=(6, 5)) dt_viz = DiscriminationThreshold(dt) dt_viz.fit(X, y) dt_viz.poof() .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter12/",
            "relUrl": "/chapter12/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "11장 모델 선택",
            "content": "11.1 &#44160;&#51613; &#44257;&#49440; . from sklearn.ensemble import RandomForestClassifier from yellowbrick.model_selection import ( ValidationCurve ) fig, ax = plt.subplots(figsize=(6, 4)) vc_viz = ValidationCurve( RandomForestClassifier(n_estimators=100), param_name=&quot;max_depth&quot;, param_range=np.arange(1, 11), cv=10, n_jobs=-1, ) vc_viz.fit(X, y) vc_viz.poof() . 11.2 &#54617;&#49845; &#44257;&#49440; . from yellowbrick.model_selection import ( LearningCurve, ) fig, ax = plt.subplots(figsize=(6, 4)) lc3_viz = LearningCurve( RandomForestClassifier(n_estimators=100), cv=10, ) lc3_viz.fit(X, y) lc3_viz.poof() .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter11/",
            "relUrl": "/chapter11/",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "10장 분류",
            "content": "10.1 &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . from sklearn.linear_model import ( LogisticRegression, ) lr = LogisticRegression(random_state=42) lr.fit(X_train, y_train) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=42, solver=&#39;warn&#39;, tol=0.0001, verbose=0, warm_start=False) . lr.score(X_test, y_test) . 0.8015267175572519 . lr.predict(X.iloc[[0]]) . array([0]) . lr.predict_proba(X.iloc[[0]]) . array([[0.89709139, 0.10290861]]) . lr.predict_log_proba(X.iloc[[0]]) . array([[-0.10859754, -2.27391397]]) . lr.decision_function(X.iloc[[0]]) . array([-2.16531643]) . lr.intercept_ . array([1.22466932]) . import numpy as np def inv_logit(p): return np.exp(p) / (1 + np.exp(p)) inv_logit(lr.intercept_) . array([0.77288422]) . cols = X.columns for col, val in sorted(zip(cols, lr.coef_[0]), key=lambda x: x[1], reverse=True): print(f&quot;{col:10}{val:10.3f} {inv_logit(val):10.3f}&quot;) . fare 0.104 0.526 parch -0.063 0.484 sibsp -0.273 0.432 age -0.295 0.427 embarked_Q -0.495 0.379 embarked_S -0.508 0.376 pclass -0.738 0.323 sex_male -2.408 0.083 . from yellowbrick.features.importances import ( FeatureImportances, ) fig, ax = plt.subplots(figsize=(6, 4)) fi_viz = FeatureImportances(lr) fi_viz.fit(X, y) fi_viz.poof() . 10.2 &#45208;&#51060;&#48652; &#48288;&#51060;&#51592; . from sklearn.naive_bayes import GaussianNB nb = GaussianNB() nb.fit(X_train, y_train) . GaussianNB(priors=None, var_smoothing=1e-09) . nb.score(X_test, y_test) . 0.7557251908396947 . nb.predict(X.iloc[[0]]) . array([0]) . nb.predict_proba(X.iloc[[0]]) . array([[0.95955327, 0.04044673]]) . nb.predict_log_proba(X.iloc[[0]]) . array([[-0.04128744, -3.20776959]]) . 10.3 &#49436;&#54252;&#53944; &#48289;&#53552; &#47672;&#49888; . from sklearn.svm import SVC svc = SVC(random_state=42, probability=True) svc.fit(X_train, y_train) . SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=True, random_state=42, shrinking=True, tol=0.001, verbose=False) . svc.score(X_test, y_test) . 0.806615776081425 . svc.predict(X.iloc[[0]]) . array([0]) . svc.predict_proba(X.iloc[[0]]) . array([[0.84850738, 0.15149262]]) . svc.predict_log_proba(X.iloc[[0]]) . array([[-0.1642765 , -1.88721835]]) . 10.4 K-&#52572;&#44540;&#51217; &#51060;&#50883; . from sklearn.neighbors import ( KNeighborsClassifier, ) knc = KNeighborsClassifier() knc.fit(X_train, y_train) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . knc.score(X_test, y_test) . 0.7684478371501272 . knc.predict(X.iloc[[0]]) . array([0]) . knc.predict_proba(X.iloc[[0]]) . array([[0.8, 0.2]]) . 10.5 &#46356;&#49884;&#51204; &#53944;&#47532; . from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier( random_state=42, max_depth=3 ) dt.fit(X_train, y_train) . DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=42, splitter=&#39;best&#39;) . dt.score(X_test, y_test) . 0.8142493638676844 . dt.predict(X.iloc[[0]]) . array([0]) . dt.predict_proba(X.iloc[[0]]) . array([[0.87954545, 0.12045455]]) . dt.predict_log_proba(X.iloc[[0]]) . array([[-0.12835003, -2.11648281]]) . import pydotplus from io import StringIO from sklearn.tree import export_graphviz from IPython.display import Image dot_data = StringIO() tree.export_graphviz( dt, out_file=dot_data, feature_names=X.columns, class_names=[&quot;Died&quot;, &quot;Survived&quot;], filled=True, ) g = pydotplus.graph_from_dot_data(dot_data.getvalue()) Image(g.create_png()) . from dtreeviz.trees import dtreeviz viz = dtreeviz( dt, X, y, target_name=&quot;survived&quot;, feature_names=X.columns, class_names=[&quot;died&quot;, &quot;survived&quot;], ) viz . G cluster_legend node2 node5 leaf3 node2-&gt;leaf3 leaf4 node2-&gt;leaf4 leaf6 node5-&gt;leaf6 leaf7 node5-&gt;leaf7 node1 node1-&gt;node2 node1-&gt;node5 node8 node9 node12 leaf10 node9-&gt;leaf10 leaf11 node9-&gt;leaf11 leaf13 node12-&gt;leaf13 leaf14 node12-&gt;leaf14 node8-&gt;node9 node8-&gt;node12 node0 node0-&gt;node1 &lt; node0-&gt;node8 &#8805; legend for col, val in sorted(zip(X.columns, dt.feature_importances_), key=lambda x: x[1], reverse=True)[:5]: print(f&quot;{col:10}{val:10.3f}&quot;) . sex_male 0.607 pclass 0.248 sibsp 0.052 fare 0.050 age 0.043 . from yellowbrick.features.importances import ( FeatureImportances, ) fig, ax = plt.subplots(figsize=(6, 4)) fi_viz = FeatureImportances(dt) fi_viz.fit(X, y) fi_viz.poof() . 10.6 &#47004;&#45924;&#54252;&#47112;&#49828;&#53944; . from sklearn.ensemble import ( RandomForestClassifier, ) rf = RandomForestClassifier(random_state=42) rf.fit(X_train, y_train) . RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) . rf.score(X_test, y_test) . 0.7659033078880407 . rf.predict(X.iloc[[0]]) . array([0]) . rf.predict_proba(X.iloc[[0]]) . array([[1., 0.]]) . rf.predict_log_proba(X.iloc[[0]]) . array([[ 0., -inf]]) . for col, val in sorted(zip(X.columns, rf.feature_importances_), key=lambda x: x[1], reverse=True)[:5]: print(f&quot;{col:10}{val:10.3f}&quot;) . age 0.280 fare 0.274 sex_male 0.230 pclass 0.086 sibsp 0.053 . 10.7 XGBoost . import xgboost as xgb xgb_class = xgb.XGBClassifier(random_state=42) xgb_class.fit( X_train, y_train, early_stopping_rounds=10, eval_set=[(X_test, y_test)], ) . [0] validation_0-error:0.188295 Will train until validation_0-error hasn&#39;t improved in 10 rounds. [1] validation_0-error:0.188295 [2] validation_0-error:0.188295 [3] validation_0-error:0.188295 [4] validation_0-error:0.188295 [5] validation_0-error:0.188295 [6] validation_0-error:0.203562 [7] validation_0-error:0.203562 [8] validation_0-error:0.203562 [9] validation_0-error:0.203562 [10] validation_0-error:0.203562 Stopping. Best iteration: [0] validation_0-error:0.188295 . XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=100, n_jobs=1, nthread=None, objective=&#39;binary:logistic&#39;, random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1) . xgb_class.score(X_test, y_test) . 0.811704834605598 . xgb_class.predict(X.iloc[[0]]) . array([0]) . xgb_class.predict_proba(X.iloc[[0]]) . array([[0.53754187, 0.46245816]], dtype=float32) . for col, val in sorted(zip(X.columns, xgb_class.feature_importances_,), key=lambda x: x[1], reverse=True)[:5]: print(f&quot;{col:10}{val:10.3f}&quot;) . sex_male 0.665 pclass 0.155 sibsp 0.069 embarked_S 0.042 age 0.038 . fig, ax = plt.subplots(figsize=(6, 4)) xgb.plot_importance(xgb_class, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa1d6027b50&gt; . fig, ax = plt.subplots(figsize=(6, 4)) fi_viz = FeatureImportances(xgb_class) fi_viz.fit(X, y) fi_viz.poof() . booster = xgb_class.get_booster() print(booster.get_dump()[0]) . 0:[sex_male&lt;1] yes=1,no=2,missing=1 1:[pclass&lt;0.23096557] yes=3,no=4,missing=3 3:[fare&lt;-0.142878294] yes=7,no=8,missing=7 7:leaf=0.132530123 8:leaf=0.184 4:[fare&lt;-0.195437849] yes=9,no=10,missing=9 9:leaf=0.0245989319 10:leaf=-0.145945951 2:[age&lt;-1.49289274] yes=5,no=6,missing=5 5:[sibsp&lt;1.81278062] yes=11,no=12,missing=11 11:leaf=0.135483876 12:leaf=-0.150000006 6:[pclass&lt;-0.957598865] yes=13,no=14,missing=13 13:leaf=-0.0666666701 14:leaf=-0.148717955 . 1 / (1 + np.exp(-1 * 0.1238)) . 0.5309105310475829 . fig, ax = plt.subplots(figsize=(50, 50)) xgb.plot_tree(xgb_class, ax=ax, num_trees=0) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa1cd19e250&gt; . import xgbfir xgbfir.saveXgbFI( xgb_class, feature_names=X.columns, OutputXlsxFile=&quot;fir.xlsx&quot;, ) pd.read_excel(&quot;/content/fir.xlsx&quot;).head(3).T . 0 1 2 . Interaction sex_male | pclass | fare | . Gain 2026.35 | 743.409 | 711.231 | . FScore 48 | 53 | 279 | . wFScore 39.4676 | 22.203 | 121.88 | . Average wFScore 0.822241 | 0.418924 | 0.436845 | . Average Gain 42.2157 | 14.0266 | 2.54922 | . Expected Gain 2016.01 | 298.442 | 291.862 | . Gain Rank 1 | 2 | 3 | . FScore Rank 4 | 3 | 1 | . wFScore Rank 3 | 4 | 1 | . Avg wFScore Rank 1 | 7 | 6 | . Avg Gain Rank 1 | 2 | 5 | . Expected Gain Rank 1 | 2 | 3 | . Average Rank 1.83333 | 3.33333 | 3.16667 | . Average Tree Index 36.8333 | 22.9434 | 54.3405 | . Average Tree Depth 0.375 | 1.20755 | 1.42652 | . pd.read_excel( &quot;fir.xlsx&quot;, sheet_name=&quot;Interaction Depth 1&quot;, ).head(2).T . 0 1 . Interaction pclass|sex_male | age|sex_male | . Gain 3301.86 | 1378.75 | . FScore 38 | 15 | . wFScore 15.9816 | 8.5093 | . Average wFScore 0.420569 | 0.567287 | . Average Gain 86.8911 | 91.9168 | . Expected Gain 1375.25 | 885.229 | . Gain Rank 1 | 2 | . FScore Rank 4 | 12 | . wFScore Rank 4 | 9 | . Avg wFScore Rank 11 | 6 | . Avg Gain Rank 2 | 1 | . Expected Gain Rank 1 | 2 | . Average Rank 3.83333 | 5.33333 | . Average Tree Index 15.9474 | 32.4 | . Average Tree Depth 1.02632 | 1 | . pd.read_excel( &quot;fir.xlsx&quot;, sheet_name=&quot;Interaction Depth 2&quot;, ).head(1).T . 0 . Interaction fare|pclass|sex_male | . Gain 4891.87 | . FScore 44 | . wFScore 7.8619 | . Average wFScore 0.178679 | . Average Gain 111.179 | . Expected Gain 870.56 | . Gain Rank 1 | . FScore Rank 1 | . wFScore Rank 5 | . Avg wFScore Rank 31 | . Avg Gain Rank 2 | . Expected Gain Rank 2 | . Average Rank 7 | . Average Tree Index 16.8864 | . Average Tree Depth 2 | . pd.read_excel( &quot;fir.xlsx&quot;, sheet_name=&quot;Interaction Depth 2&quot;, )[[&quot;Interaction&quot;, &quot;Gain&quot;]].head() . Interaction Gain . 0 fare|pclass|sex_male | 4891.867318 | . 1 age|pclass|sex_male | 2999.230953 | . 2 age|sex_male|sibsp | 1518.797561 | . 3 age|fare|sex_male | 334.379201 | . 4 embarked_S|pclass|sex_male | 225.302789 | . 10.8 LightGBM&#51012; &#49324;&#50857;&#54620; &#44536;&#47000;&#46356;&#50616;&#53944; &#48512;&#49828;&#54021; . import lightgbm as lgb lgbm_class = lgb.LGBMClassifier( random_state=42 ) lgbm_class.fit(X_train, y_train) . LGBMClassifier(boosting_type=&#39;gbdt&#39;, class_weight=None, colsample_bytree=1.0, importance_type=&#39;split&#39;, learning_rate=0.1, max_depth=-1, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31, objective=None, random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0, subsample_for_bin=200000, subsample_freq=0) . lgbm_class.score(X_test, y_test) . 0.806615776081425 . lgbm_class.predict(X.iloc[[0]]) . array([0]) . lgbm_class.predict_proba(X.iloc[[0]]) . array([[0.98090161, 0.01909839]]) . for col, val in sorted(zip(cols, lgbm_class.feature_importances_), key=lambda x: x[1], reverse=True)[:5]: print(f&quot;{col:10}{val:10.3f}&quot;) . fare 1285.000 age 1198.000 sex_male 113.000 pclass 112.000 sibsp 99.000 . fig, ax = plt.subplots(figsize=(6, 4)) lgb.plot_importance(lgbm_class, ax=ax) fig.tight_layout() . fig, ax = plt.subplots(figsize=(200, 200)) lgb.plot_tree(lgbm_class, tree_index=0, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa1ccec1050&gt; . 10.9 TPOT . from tpot import TPOTClassifier tc = TPOTClassifier(generations=2) tc.fit(X_train, y_train) tc.score(X_test, y_test) . /usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+. warnings.warn(msg, category=DeprecationWarning) Version 0.9.5 of tpot is outdated. Version 0.11.7 was released Wednesday January 06, 2021. . 0.7888040712468194 . tc.predict(X.iloc[[0]]) . array([0]) . tc.predict_proba(X.iloc[[0]]) . array([[0.92383425, 0.07616575]]) . tc.export(&quot;tpot_exported_pipeline.py&quot;) . True . !cat ./tpot_exported_pipeline.py . import numpy as np import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split # NOTE: Make sure that the class is labeled &#39;target&#39; in the data file tpot_data = pd.read_csv(&#39;PATH/TO/DATA/FILE&#39;, sep=&#39;COLUMN_SEPARATOR&#39;, dtype=np.float64) features = tpot_data.drop(&#39;target&#39;, axis=1).values training_features, testing_features, training_target, testing_target = train_test_split(features, tpot_data[&#39;target&#39;].values, random_state=None) # Average CV score on the training set was:0.8209729151817534 exported_pipeline = RandomForestClassifier(bootstrap=False, criterion=&#34;entropy&#34;, max_features=0.5, min_samples_leaf=5, min_samples_split=18, n_estimators=100) exported_pipeline.fit(training_features, training_target) results = exported_pipeline.predict(testing_features) .",
            "url": "https://deep-diver.github.io/pocket-ml-reference-korean/chapter10/",
            "relUrl": "/chapter10/",
            "date": " • Mar 9, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://deep-diver.github.io/pocket-ml-reference-korean/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://deep-diver.github.io/pocket-ml-reference-korean/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}